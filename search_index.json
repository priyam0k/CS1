[["index.html", "Actuarial Statistics (Priyam’s Fork) Introduction Motivation", " Actuarial Statistics (Priyam’s Fork) Priyam 29 July, 2025 Introduction This book is a modified version of the original work by Alex Garbiak. As an adapted version, there may be changes, bugs, typos, or errors. Please report any issues you find here. Motivation The purpose of this book is to build upon the original work for studying Actuarial Statistics (CS1) from the Institute and Faculty of Actuaries by providing updated R code examples across the following topic areas: Random variables and distributions Data analysis Statistical inference Regression theory and applications Bayesian statistics "],["r-setup.html", "Chapter 1 R Setup 1.1 Preparing your environment for R 1.2 Basic interations with R 1.3 Functions in R 1.4 Data structures in R 1.5 Logical expressions in R 1.6 Extending R with packages 1.7 Importing data", " Chapter 1 R Setup 1.1 Preparing your environment for R The Institute and Faculty of Actuaries have provided their own guide to getting up and running with R. The steps to have R working is dependant on your operating system. The following resources should allow for your local installation of R to be relatively painless: Download and install R from CRAN1. Download and install an integrated development environment, a strong recommendation is RStudio Desktop. 1.2 Basic interations with R R is case-sensitive! We add comments to our R code using the # symbol on any line. A key concept when working with R is that the preference is to work with vectorised operations (over concepts like for loops). As an example we start with 1:10 which uses the colon operator (:) to generate a sequence starting with 1 and ending with 10 in steps of 1. The output is a numeric vector of integers. Let’s see this in R: # This is the syntax for comments in R (1:10) + 2 # Notice how we add element-wise in R ## [1] 3 4 5 6 7 8 9 10 11 12 At the most basic level, R vectors can be of atomic modes: integer, numeric (equivalently, double), logical which take on the Boolean types: TRUE or FALSE and can be coerced into integers as 1 and 0 respectively, character which will be apparent in R with the wrapper ““, complex, and raw This book focuses on using R to solve actuarial statistical problems and will not explore the depths of the R language2. R has the usual arithmetic operators you’d expect with any programming language: +, -, *, / for addition, subtraction, multiplication and division, ^ for exponentiation, %% for modulo arithmetic (remainder after division) %/% for integer division We assign values to variables using the &lt;- (“assignment”) operator3. x &lt;- 1:10 y &lt;- x + 2 x &lt;- x + x # Notice that we can re-assign values to variables z &lt;- x + 2 y ## [1] 3 4 5 6 7 8 9 10 11 12 z ## [1] 4 6 8 10 12 14 16 18 20 22 Even though \\(z\\) is assigned the same way as we assigned \\(y\\), note that \\(y \\neq z\\) so execution order matters in R. All of \\(x\\), \\(y\\) and \\(z\\) are vectors in R. 1.3 Functions in R We can add functions to R via the format function_name(arguments = values, ...): # c() is the &quot;combine&quot; function, used often to create vectors # Note we can also nest functions within functions x &lt;- c(1:3, 6:20, 21:42, c(43, 44)) # Another function with arguments: y &lt;- sample(x, size = 3) y ## [1] 7 44 6 There are a lot of in-built functions in R that we may need: factorial(x) choose(n, k) - for binomial coefficients exp(x) log(x) - by default in base \\(e\\) gamma(x) abs(x) - absolute value sqrt(x) sum(x) mean(x) median(x) var(x) sd(x) quantile(x, 0.75) set.seed(seed) - for reproducibility of random number generation sample(x, size) R has an in-built help function ? which can be used to read the documentation on any function as well as topic areas. For example have a look at ?Special for more details about in-built R functions for the beta and gamma functions. 1.4 Data structures in R We have already seen vectors as a data structure that is very common in R. We can identify the structure of an R “object” using the str(object) function. Matrices Next we introduce the matrix structure. When interacting with matrices in R it is important to note that matrix multiplication requires the %*% syntax: first_matrix &lt;- matrix(1:9, byrow = TRUE, nrow = 3) first_matrix %*% first_matrix ## [,1] [,2] [,3] ## [1,] 30 36 42 ## [2,] 66 81 96 ## [3,] 102 126 150 Dataframes A data.frame is a very popular data structure used in R. Each input variable has to have the same length but can be of different types (strings, integers, booleans, etc.). # Input vectors for the data.frame name &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;) surface_gravity &lt;- c(0.38, 0.904, 1, 0.3794, 2.528, 1.065, 0.886, 1.14) # Create a data.frame from the vectors solar_system &lt;- data.frame(name, surface_gravity) str(solar_system) ## &#39;data.frame&#39;: 8 obs. of 2 variables: ## $ name : chr &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; ... ## $ surface_gravity: num 0.38 0.904 1 0.379 2.528 ... Lists A list is a versatile data structure in R as their elements can be of any type, including lists themselves. In fact a data.frame is a specific implementation of a list which allows columns in a data.frame to have different types, unlike a matrix. We will come across a number of functions that return a list type whilst working with actuarial statistics in R. For example when we look at linear models we will make use of the lm(formula, data, ...) function which returns a list. # Use Orange dataset df &lt;- Orange # Fit a linear model to predict circumference from age fitted_lm &lt;- lm(circumference ~ age, df) # Size of the list length(fitted_lm) ## [1] 12 # Element names names(fitted_lm) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; We can access elements in the list using subsetting, noting the use of the [[ operator. Here we subset on “age” within the “coefficient” element in the list we called “fitted_lm”: # Select [[1]] 1st element in the list, sub-select [2] 2nd element from that fitted_lm[[1]][2] ## age ## 0.1067703 # fitted_lm$coefficient is a shorthand for fitted_lm[[&quot;coefficient&quot;]] fitted_lm$coefficients[2] ## age ## 0.1067703 # Select element using matching character vector &quot;age&quot; fitted_lm$coefficients[&quot;age&quot;] ## age ## 0.1067703 # Select elements using matching character vectors fitted_lm[[&quot;coefficients&quot;]][&quot;age&quot;] ## age ## 0.1067703 1.5 Logical expressions in R R has built in logic expressions: Operator Description &lt; (&lt;=) less than (or equal to) &gt; (&gt;=) greater than (or equal to) == exactly equal to ! NOT &amp; AND (element-wise) | OR (element-wise) != not equal to We can use logical expressions to effectively filter data via subsetting the data using the [...] syntax: x &lt;- 1:10 x[x != 5 &amp; x &lt; 7] ## [1] 1 2 3 4 6 We can select objects using the $ symbol (see ?Extract for more help): #data.frame[rows to select, columns to select] solar_system[solar_system$name == &quot;Jupiter&quot;, c(1:2)] ## name surface_gravity ## 5 Jupiter 2.528 1.6 Extending R with packages We can extend R’s functionality by loading packages: # Load the ggplot2 package library(ggplot2) Did you get an error from R trying this? To load packages they need to be installed using install.packages(&quot;package name&quot;). 1.7 Importing data R can import a wide variety of file formats, including: .csv .RData .txt We can import these using read.csv(file, header, sep), load(file) and read.table(file, header, sep) respectively, where: file is the file name to be read, header is a logical value to determine if the first line contains the variable names, and sep is the specified field separator character, e.g. for .csv files by default sep = \",\" given it is a comma separated value file type. CRAN is the The Comprehensive R Archive Network - read more on the CRAN website↩︎ I fear this is already too indepth for “basic interactions with R” but for those that want to jump down the rabbit hole, see Hadley Wickham’s book Advanced R.↩︎ We can also assign values using the more familiar = symbol. In general this is discouraged, listen to Hadley Wickham.↩︎ "],["discrete-probability-distributions.html", "Chapter 2 Discrete Probability Distributions Learning Objectives Theory 2.1 In-built probability distributions Discrete probability distributions covered 2.2 Geometric distribution 2.3 Binomial distribution 2.4 Negative binomial distribution 2.5 Hypergeometric distribution 2.6 Poisson distribution 2.7 Uniform distribution R Practice", " Chapter 2 Discrete Probability Distributions Learning Objectives Define and explain the key characteristics of the discrete distributions: geometric, binomial, negative binomial, hypergeometric, Poisson and uniform on a finite set. Evaluate probabilities and quantiles associated with such distributions. Theory R was designed to be used for statistical computing - so it handles randomness well! Using R we can guarantee reproducibility (and enhance sharability) by using the function set.seed(seed) where seed is a single value integer. Using this approach we guarantee the generation of the same sequence of random numbers everytime we call this function. Use ?set.seed to learn more about this function. Let’s see set.seed in action: # We make five random draws from the integer range [1, 10] # We cannot guarantee reproducing this outcome when sharing the code: sample(1:10, 5) ## [1] 10 8 4 2 6 # Now we set a seed value before making the five random draws # We guarantee a fixed output which enhances reproducibility and sharability: set.seed(42) # Fixes result # Using set.seed(42) we guarantee five random draws within the integer range [1, 10] will be: # 1, 5, 10, 8, 2 sample(1:10, 5) ## [1] 1 5 10 8 2 # We can re-initialise the seed with the same value # Observe that the same sequence of random numbers are generated: set.seed(42) # Fixes result # Guarantee we draw 1, 5, 10, 8, 2 again sample(1:10, 5) ## [1] 1 5 10 8 2 2.1 In-built probability distributions R has in-built functions for probability distributions: d&lt;distribution-name&gt; \\(:=\\) density (“PDF”), i.e. \\(f_X(x)\\) p&lt;distribution-name&gt; \\(:=\\) probability distribution cumulative function (“CDF”), i.e. \\(F_X(x) =\\boldsymbol{P}(X \\leq x)\\) q&lt;distribution-name&gt; \\(:=\\) quantile function, i.e. return \\(x\\) such that \\(\\boldsymbol{P}(X \\leq x) = p\\) r&lt;distribution-name&gt; \\(:=\\) random deviates, i.e. (psuedo) random number generator for a given distribution Where &lt;distribution-name&gt; \\(=\\) Normal, uniform, lognormal, Student’s \\(t\\), Poisson, binormal, Weibull … see ?distributions() for more information To give some quick examples (we will explore these in more detail later in this chapter and the next chapter): R Code Definition rnorm(1) Generates \\(x_1\\) where \\(X \\sim \\mathcal{N}(0,\\,1)\\) rnorm(y, mean=10, sd=2) Generates \\(\\{y_1,\\,y_2,\\,\\dots\\}\\) with \\(Y \\sim \\mathcal{N}(10,\\,2^2)\\) runif(3, min=5, max=10) Generates \\(\\{z_1,\\,z_2,\\,z_3\\}\\) where \\(Z \\sim \\mathcal{U}(5,\\,10)\\) dbinom(4, size=5, prob=0.5) Computes \\(\\boldsymbol{P}(X = 4)\\) where \\(X \\sim Bin(5,\\,0.5)\\) pgamma(0.2, shape=2, rate=2) Computes \\(F_Y(0.2)\\) where \\(Y \\sim \\mathcal{\\Gamma}(2,\\,2)\\), i.e. \\(\\boldsymbol{P}(Y\\leq 0.2)\\) qexp(0.5, rate = 2) Determines smallest value of \\(z\\) for \\(\\boldsymbol{P}(Z \\leq z) = 0.5\\) where \\(Z \\sim Exp(2)\\) Discrete probability distributions covered We will consider how to interact with the following discrete probability distributions in R: Geometric Binomial Negative binomial Hypergeometric Poisson Uniform (on a finite set) For each distribution above we will determine how to calculate: A random deviate following the discrete distribution \\(X\\), The probability mass function (“PMF”), \\(P(X = k)\\) for distribution \\(X\\) and \\(-\\infty &lt; k &lt; \\infty\\) (noting the PMF \\(= 0\\) for most values of \\(k\\)), The cumulative distribution function (“CDF”), \\(P(X \\leq k)\\), A range of PMFs, i.e. \\(P(k_1 \\leq X \\leq k_2)\\), and The quantile function to find \\(k\\) representing the value such that \\(P(X \\leq k) = p\\), i.e. the pth percentile. We will finish off with a plot of the distribution. 2.2 Geometric distribution The geometric probability distribution (\\(X\\)) can be defined by the number of failures (\\(k\\)) in Bernoulli trials before achieving a success. A Bernoulli trial is a random experiment with exactly two outcomes, {“success”, “failure”}, in which the probability of success is constant in every experiment. More formally, if we have \\(k \\in \\{0,\\,1,\\,2,\\, \\dots \\}\\) independent, identically distributed (“i.i.d.”) Bernoulli trials before a “success” with the \\(P(\\)“success”\\()\\) on each trial defined as \\(p\\), then: \\[P(X = k) = (1 - p)^{k}p\\] We have \\(X \\sim Geo(p)\\) with \\(p \\in (0, 1]\\). In R we can generate random deviates following a geometric distribution using the function rgeom(n, prob) where: n is the number of random deviates we want to generate, and prob is the probability of success in any given Bernoulli trial. # Guarantee reproducibility set.seed(42) # Generate 5 random deviates following X~Geo(0.25) rgeom(5, 0.25) ## [1] 1 1 0 1 2 If we wanted to find the xth percentile of \\(X \\sim Geo(p)\\) we would use the quantile function, qgeom(p, prob). # Find the 99th percentile of X~Geo(0.25) percentile_99 &lt;- qgeom(0.99, 0.25) paste0( &quot;The 99th percentile of X~Geo(0.25) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of X~Geo(0.25) is 16.&quot; If we wish to find \\(P(X \\leq k)\\) then we need to use the function for the cumulative distribution function, pgeom(q, prob) where: q equates to \\(k\\) failures before success in a sequence of i.i.d. Bernoulli trials, prob is the probability of success in any given Bernoulli trial. # Find P(X &lt;= 7) with X~Geo(0.25) prob_geom &lt;- pgeom(7, 0.25) paste0( &quot;P(X &lt;= 7) with X~Geo(0.25) is &quot;, format(prob_geom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(X &lt;= 7) with X~Geo(0.25) is 0.8999.&quot; We can find \\(P(X = k)\\) using the dgeom() function call: # Find P(X = 7) with X~Geo(0.25) prob_geom &lt;- dgeom(7, 0.25) We find that \\(P(X = 7) =\\) 0.0333709716796875. It helps to visualise the probability distribution. We can do this in base R with the plot() function. However for illustrative purposes we will use the ggplot2 package from the tidyverse suite of packages. ggplot2 allows for rich visualisations4 using a consistent syntax. # Plot the distribution function of X~Geo(0.25) library(ggplot2) df &lt;- data.frame(x = rgeom(1000, 0.25)) ggplot(df, aes(x=x)) + geom_histogram(binwidth = 0.5) 2.3 Binomial distribution The binomial probability distribution (\\(Y\\)) can be defined by the number of successes in a sequence of \\(n\\) i.i.d. Bernoulli trials. A Bernoulli trial is a random experiment with exactly two outcomes, {“success”, “failure”}, wherein the probability of success is \\(p\\). We often state the probability of failure as \\(q = 1 - p\\) for convenience. More formally, if we have \\(k \\in \\{0,\\,1,\\,2,\\, \\dots,\\, n \\}\\) successes given \\(n\\) i.i.d. Bernoulli trials with the \\(P(\\)“success”\\()\\) on each trial defined as \\(p\\), then: \\[P(Y = k) = \\binom{n}{k}p^k(1 - p)^{n - k}\\] It can help to think of this as: \\(p^k\\) chance of \\(k\\) successes, \\((1 - p)^{n - k}\\) chance of \\(n - k\\) failures, the ordering of the \\(k\\) successes can occur anywhere within the \\(n\\) trials, hence \\(\\binom{n}{k}\\). We can generate random deviates following the binomial distribution \\(Y \\sim Bin(n,\\,p)\\) using rbinom(n, size, prob) where: n is the number of random deviates we want to generate, size is the number of i.i.d. Bernoulli trials performed, and prob is the probability of success in any given trial. # Generate 5 random deviates on Y~Bin(10, 0.55) set.seed(42) rbinom(5, 10, 0.55) ## [1] 3 3 6 4 5 To find \\(P(Y \\leq k)\\) we can use the function pbinom(p, size, prob) where: p is the probability of interest, size is the number of i.i.d. Bernoulli trials performed, and prob is the probability of success in any given trial. Suppose we were told the probability of having a boy at birth was 0.51314515. Additionally, suppose we were told that there were 1,264 births6. Given this, we are curious to know what is the probability of there being less than 632 boys (i.e. &lt; 50%) amongst the new babies. # Find P(Y &lt;= 632) with Y~Binom(1264, 0.5131451) prob_binom &lt;- pbinom(632, 1264, 0.5131451) paste0( &quot;P(Y &lt;= 632) with Y~Binom(1264, 0.5131451) is &quot;, format(prob_binom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y &lt;= 632) with Y~Binom(1264, 0.5131451) is 0.1822.&quot; To find \\(P(Y = k)\\) we can use the function dbinom(x, size, prob) where: x is the quantile of interest, size is the number of i.i.d Bernoulli trials performed, and prob is the probability of success in any given trial. Throwing a fair coin, we are curious to compute the probability of observing 4 heads in a sequence of 5 tosses: # Find P(Y = 4) where Y~Binom(5, 0.5) prob_binom &lt;- dbinom(4, 5, 0.5) paste0( &quot;P(Y = 4) with Y~Binom(5, 0.5) is &quot;, format(prob_binom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y = 4) with Y~Binom(5, 0.5) is 0.1562.&quot; If we wanted to find \\(P(k_1 \\leq Y \\leq k_2)\\) we can also use the function dbinom(x, size, prob) with x entered as a vector and summing over the output. Continuing the fair coin example, we are curious what is the probability of observing at least 1 head and at most 3 heads in a sequence of 5 tosses: # Find P(1 &lt;= Y &lt;= 3) where Y~Binom(5, 0.5) prob_binom &lt;- sum(dbinom(1:3, 5, 0.5)) paste0( &quot;P(1 &lt;= Y &lt;= 3) with Y~Binom(5, 0.5) is &quot;, format(prob_binom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(1 &lt;= Y &lt;= 3) with Y~Binom(5, 0.5) is 0.7812.&quot; When we are interested in finding the yth percentile we can use the quantile function, qbinom(p, size, prob) where: p is the percentile of interest, size is the number of i.i.d Bernoulli trials performed, and prob is the probability of success in any given Bernoulli trial. Continuing the baby boys example, we now are curious what is the 99th percentile of baby births being boys. Recall that the probability (p) of having a boy at birth was 0.5131451 and that there were 1,264 births (n) in Guildford in 2019. # Find the 99th percentile of Y~Binom(1264, 0.5131451) percentile_99 &lt;- qbinom(0.99, 1264, 0.5131451) paste0( &quot;The 99th percentile of Y~Binom(1264, 0.5131451) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Y~Binom(1264, 0.5131451) is 690.&quot; Finally, we finish with a plot of binomial distributions and note that with large \\(n\\) the shape of the distribution begins to form the normal distribution (as long as \\(p\\) is not near the extremes of 0 or 1). # Plot the distribution function of X, Y, Z with: # X,Y,Z~Binom(n, p) for various {n, p} library(ggplot2) set.seed(42) df &lt;- data.frame( dist_types = factor( rep(c( &quot;n = 20, p = 0.5&quot;, &quot;n = 20, p = 0.7&quot;, &quot;n = 40, p = 0.5&quot; ), each = 1000 ) ), dist_values = c( rbinom(1000, 20, 0.5), rbinom(1000, 20, 0.7), rbinom(1000, 40, 0.5) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + ggtitle(&quot;Histogram of binomial distributions for various n, p&quot;) 2.4 Negative binomial distribution The negative binomial distribution (\\(Z\\)) can be defined by the number of failures in a sequence of \\(n\\) independent, identically distributed (“i.i.d.”) Bernoulli trials before a specified number of successes, \\(r\\), occurs. More formally, if we have, \\(r &gt;0\\) number of successes, \\(k\\) number of failures, \\(p\\) probability of success then: \\[P(Z = k) = \\binom{k+r-1}{k}(1-p)^{k}p^{r}\\] We can see that with \\(r = 1\\), the binomial distribution is a special case of the more general negative binomial distribution. Let us begin by generate random deviates from the negative binomial distribution, \\(Z \\sim NegBin(r, p)\\), using rnbinom(n, size, prob) where: n is the number of random deviates we want to generate, size is the target number of successes from i.i.d. Bernoulli trials, and prob is the probability of success in any given trial # Generate random deviates from the negative binomial distribution set.seed(42) rnbinom(5, 10, 0.5) ## [1] 12 6 10 14 16 To find \\(P(Z \\leq k)\\) we can use the function pnbinom(q, size, prob) where: q is the quantile of interest, size is the target number of successes from i.i.d. Bernoulli trials, prob is the probability of success in any given trial. # Find P(Z &lt;= 2) with Z~NegBin(5, 0.5) prob_nbinom &lt;- pnbinom(2, 5, 0.5) paste0( &quot;P(Z &lt;= 2) with Z~NegBin(5, 0.5) is &quot;, format(prob_nbinom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z &lt;= 2) with Z~NegBin(5, 0.5) is 0.2266.&quot; To find \\(P(Z = k)\\) we can use the function dnbinom(x, size, prob) where: x is the quantile of interest, size is the target number of successes from i.i.d. Bernoulli trials, and prob is the probability of success in any given trial. # Find P(Z = 2) where Z~NegBin(5, 0.5) prob_nbinom &lt;- dnbinom(2, 5, 0.5) paste0( &quot;P(Z = 2) with Z~NegBin(5, 0.5) is &quot;, format(prob_nbinom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z = 2) with Z~NegBin(5, 0.5) is 0.1172.&quot; We can also calculate \\(P(k_1 \\leq Z \\leq k_2)\\) with dnbinom(x, size, prob) by entering x as a vector input and summing over the output. # Find P(1 &lt;= Z &lt;= 3) with Z~NegBin(5, 0.5) prob_nbinom &lt;- sum(dnbinom(1:3, 5, 0.5)) paste0( &quot;P(1 &lt;= Z &lt;= 3) with Z~NegBin(5, 0.5) is &quot;, format(prob_nbinom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(1 &lt;= Z &lt;= 3) with Z~NegBin(5, 0.5) is 0.332.&quot; When we are interested in finding the zth percentile we can make use of the quantile function, qnbinom(p, size, prob) where: p is the percentile of interest, size is the target number of successes from i.i.d. Bernoulli trials, and prob is the probability of success in any given trial. # Find the 99th percentile of Z~NegBin(100, 0.5) percentile_99 &lt;- qnbinom(0.99, 100, 0.5) paste0( &quot;The 99th percentile of Z~NegBin(100, 0.5) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Z~NegBin(100, 0.5) is 135.&quot; Finally, we finish with a plot of the negative binomial distribution: # Plot the distribution function of X, Y, Z with: # X,Y,Z~NegBin(r, p) for various {r, p} library(ggplot2) set.seed(42) df &lt;- data.frame( dist_types = factor( rep(c( &quot;r = 5, p = 0.6&quot;, &quot;r = 5, p = 0.5&quot;, &quot;r = 10, p = 0.4&quot; ), each = 1000 ) ), dist_values = c( rnbinom(1000, 5, 0.6), rnbinom(1000, 5, 0.5), rnbinom(1000, 10, 0.4) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + ggtitle(&quot;Histogram of negative binomial distributions for various r, p&quot;) 2.5 Hypergeometric distribution The hypergeometric distribution (\\(X\\)) can be defined by the probability of \\(k\\) successes in \\(n\\) draws without replacement from a finite population \\(N\\) that contains exactly \\(K\\) “success” objects/states. \\[P(X = k) = \\frac{ \\binom{K}{k} \\binom{N - K}{n - k} }{ \\binom{N}{n} }\\] We begin by generating random deviates from the hypergeometric distribution, \\(X \\sim Hyper(N, K, n)\\), using rhyper(nn, m, n, k) where: nn is number of observations we want to generate, m is number of “success” objects/states in the population \\(N\\), n is number of “failure” objects/states in the population \\(N\\), and k is the number of objects drawn / states observed Traditionally we have defined the population \\(N\\) as an urn containing m white balls and n black balls and this is how the distribution arguments are specified in the stats package. Note that m \\(= K\\) and n \\(= N - K\\). # Generate random deviates on X~Hyper(N = 100, K = 90, n = 10) set.seed(42) rhyper(5, 90, 100 - 90, 10) ## [1] 8 8 10 8 9 In the above random deviate generation we specified a 90% (\\(\\frac{90}{90 + 10}\\)) chance of drawing a “success” object and we made 10 draws from this population of 100 objects. With such a high proportion of “success” objects it was not suprising to note we generated random deviates from this distribution close to 10/10. Next we wish to find \\(P(X \\leq k)\\) which involves using phyper(q, m, n, k) where: q is the quantile of interest (representing the number of “success” objects in the finite population \\(N\\)), m is the number of “success” objects in the population \\(N\\), n is number of “failure” objects in the population \\(N\\), and k is the number of objects drawn # Find P(X &lt;= 5) with X~Hyper(N = 100, K = 90, n = 10) prob_hyper &lt;- phyper(5, 90, 100 - 90, 10) paste0( &quot;With 10 objects drawn from a total 100 possible, &quot;, &quot;P(X &lt;= 3) with X~Hyper() is &quot;, format(prob_hyper, digits = 4), &quot;.&quot; ) ## [1] &quot;With 10 objects drawn from a total 100 possible, P(X &lt;= 3) with X~Hyper() is 0.0006716.&quot; To find \\(P(X = k)\\) we can use the function dhyper(x, m, n, k) where: x is the quantile of interest (representing the number of “success” objects in the finite population \\(N\\)), m is the number of “success” objects in the population \\(N\\), n is number of “failure” objects in the population \\(N\\), and k is the number of objects drawn # Find P(X = 5) with X~Hyper(N = 100, K = 90, n = 10) prob_hyper &lt;- dhyper(5, 90, 100 - 90, 10) We can also calculate \\(P(k_1 \\leq X \\leq k_2)\\) with dhyper(x, m, n, k) by entering x as a vector input and summing over the output. # Find P(7 &lt;= X &lt;= 9) with X~Hyper(N = 100, K = 90, n = 10) prob_hyper &lt;- sum(dhyper(7:9, 90, 100 - 90, 10)) When we are interested in finding the xth percentile we can make use of the quantile function, qhyper(p, m, n, k) where: p is the percentile of interest, m is the number of “success” objects in the population \\(N\\), n is number of “failure” objects in the population \\(N\\), and k is the number of objects drawn # Find the 99th percentile of X~Hyper(N = 100, K = 90, n = 50) percentile_99 &lt;- qhyper(0.99, 90, 100 - 90, 50) paste0( &quot;When we draw 50 objects from a population of &quot;, &quot;90 &#39;success&#39; objects and 10 &#39;failure&#39; objects, &quot;, &quot;we expect the 99th percentile outcome to be &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;When we draw 50 objects from a population of 90 &#39;success&#39; objects and 10 &#39;failure&#39; objects, we expect the 99th percentile outcome to be 48.&quot; Finally, we finish with a plot of the hypergeometric distribution: # Plot the distribution function of X, Y, Z with: # X,Y,Z~Hyper(N, K, n) for various {N, K, n} library(ggplot2) set.seed(42) inputs &lt;- c( &quot;N = 500, K = 50, n = 100&quot;, &quot;N = 500, K = 60, n = 200&quot;, &quot;N = 500, K = 70, n = 300&quot; ) df &lt;- data.frame( dist_types = rep(inputs, each = 1000), dist_values = c( rhyper(1000, 50, 500 - 50, 100), rhyper(1000, 60, 500 - 60, 200), rhyper(1000, 70, 500 - 70, 300) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + scale_fill_discrete(breaks = inputs) + ggtitle(&quot;Histogram of Hypergeometric distributions for various inputs&quot;) 2.6 Poisson distribution The Poisson distribution (\\(Y\\)) is usually defined as the probability of a given number of events occurring (\\(k\\)) in a fixed interval of time where events occur with a constant mean rate (\\(\\lambda\\)) and independently of the time since the last event. More formally, for \\(\\lambda &gt; 0\\) and \\(k = 0,\\,1,\\,2,\\,\\dots\\,\\): \\[P(Y = k) = \\frac{\\lambda^k e^{-k}}{k!}\\] We can generate random deviates from the Poisson distribution \\(Y \\sim Pois(\\lambda)\\) using rpois(n, lambda) where: n is the number of random deviates we want to generate, and lambda is the constant mean rate. # Generate 5 random deviates on Y~Pois(8) set.seed(42) rpois(5, 8) ## [1] 12 13 6 11 9 To find \\(P(Y \\leq k)\\) we can use the function ppois(q, lambda) where: q is the quantile of interest, and lambda is the constant mean rate. # Find P(Y &lt;= 5) with Y~Pois(8) prob_pois &lt;- ppois(5, 8) paste0( &quot;P(Y &lt;= 5) with Y~Pois(5, 8) is &quot;, format(prob_pois, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y &lt;= 5) with Y~Pois(5, 8) is 0.1912.&quot; To find \\(P(Y = k)\\) we can use the function dpois(x, lambda) where: x is the quantile of interest, and lambda is the constant mean rate. # Find P(Y = 5) with Y~Pois(8) prob_pois &lt;- dpois(5, 8) paste0( &quot;P(Y = 5) with Y~Pois(5, 8) is &quot;, format(prob_pois, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y = 5) with Y~Pois(5, 8) is 0.0916.&quot; If we wanted to find \\(P(k_1 \\leq Y \\leq k_2)\\) we can also use the function dpois(x, lambda) with x entered as a vector and summing over the output. # Find P(4 &lt;= Y &lt;= 6) where Y~Pois(8) prob_pois &lt;- sum(dpois(4:6, 5)) paste0( &quot;P(4 &lt;= Y &lt;= 6) with Y~Pois(8) is &quot;, format(prob_pois, digits = 4), &quot;.&quot; ) ## [1] &quot;P(4 &lt;= Y &lt;= 6) with Y~Pois(8) is 0.4972.&quot; When we are interested in finding the yth percentile we can use the quantile function, qpois(p, lambda) where: p is the percentile of interest, and lambda is the constant mean rate. # Find the 99th percentile of Y~Poisson(8) percentile_99 &lt;- qpois(0.99, 8) paste0( &quot;The 99th percentile of Y~Pois(8) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Y~Pois(8) is 15.&quot; Finally, we finish with a plot of Poisson distributions. We note that for large \\(\\lambda\\) the Poisson distribution is well approximated by the normal distribution with \\(\\mu = \\lambda\\) and \\(\\sigma^2 = \\lambda\\). We can approximate the Poisson distribution with this normal distribution after allowing for a continuity correction: \\[F_{Pois}(x; \\lambda) \\approx F_{normal}(x + 0.5; \\mu = \\lambda, \\sigma^2 = \\lambda)\\] Here we have replaced \\(P(X \\leq x)\\) with \\(P(X \\leq x + 0.5)\\) on account of the continuity correction. # Plot the distribution function of X, Y, Z with: # X,Y,Z~Pois(lambda) for various lambda library(ggplot2) set.seed(42) lambdas &lt;- c( &quot;lambda = 8&quot;, &quot;lambda = 16&quot;, &quot;lambda = 32&quot; ) df &lt;- data.frame( dist_types = rep(lambdas, each = 1000), dist_values = c( rpois(1000, 8), rpois(1000, 16), rpois(1000, 32) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + scale_fill_discrete(breaks = lambdas) + ggtitle(&quot;Histogram of Poisson distributions for various lambda&quot;) 2.7 Uniform distribution Our final discrete probability distribution concerns the uniform distribution (\\(Z\\)) defined over a finite set \\({a, b}\\). This is one discrete probability distribution which is not part of the source code for the stats package in base R. We can use rdunif(n, b, a) function within the purrr package7 to generate random deviates following the discrete uniform distribution. We have: \\[P(Z = k) = \\frac{1}{b - a + 1}\\] As an example we can simulate a die throw using \\(Z \\sim \\mathcal{U}(1, 6)\\) as follows: # Generate 5 random deviates on Z~Unif(1, 6) set.seed(42) purrr::rdunif(5, 6, 1) ## [1] 1 5 1 1 2 To find \\(P(Z \\leq k)\\) we can calculate this using the CDF: \\[P(Z \\leq k) = \\frac{\\lfloor k \\rfloor - a + 1}{b - a + 1}\\] # Find P(Z &lt;= 2) with Z~Unif(1, 6) prob_uniform &lt;- (2 - 1 + 1) / (6 - 1 + 1) paste0( &quot;P(Z &lt;= 2) with Z~Unif(1, 6) is &quot;, format(prob_uniform, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z &lt;= 2) with Z~Unif(1, 6) is 0.3333.&quot; To find \\(P(Z = k)\\) we can use the PMF, \\(P(Z = k) = \\frac{1}{b - a + 1}\\): # Find P(Z = 2) with Z~Unif(1, 6) prob_uniform &lt;- 1 / (6 - 1 + 1) paste0( &quot;P(Z = 2) with Z~Unif(1, 6) is &quot;, format(prob_uniform, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z = 2) with Z~Unif(1, 6) is 0.1667.&quot; To find \\(P(k_1 \\leq Z \\leq k_2)\\) we can sum over the PMFs: # Find P(3 &lt;= Z &lt;= 5) where Z~Unif(1, 6) # P(Z = k) is constant for all k: prob_uniform &lt;- 1 / (6 - 1 + 1) # We have k = 3, 4, 5: prob_uniform &lt;- prob_uniform * 3 paste0( &quot;P(3 &lt;= Z &lt;= 5) with Z~Unif(1, 6) is &quot;, format(prob_uniform, digits = 4), &quot;.&quot; ) ## [1] &quot;P(3 &lt;= Z &lt;= 5) with Z~Unif(1, 6) is 0.5.&quot; When we are interested in finding the zth percentile we can use the quantile function \\(F^{-1}_{Z}(p)\\): \\[F^{-1}(p) = \\lfloor (b - a + 1)p \\rfloor - 1\\] # Plot the distribution function of Z~Uniform(1, 6) percentile_99 &lt;- floor((6 - 1 + 1)*0.99) paste0( &quot;The 99th percentile of Z~Unif(1, 6) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Z~Unif(1, 6) is 5.&quot; Finally, we finish with a plot of uniform distributions for various inputs. # Plot the distribution function of X, Y, Z with: # X,Y,Z~Unif(a, b) for various {a, b} library(ggplot2) set.seed(42) finite_sets &lt;- c( &quot;a = 1, b = 6&quot;, # 1 die &quot;a = 1, b = 12&quot;, # 2 dice &quot;a = 1, b = 20&quot; # d20 die from D&amp;D ) df &lt;- data.frame( dist_types = rep(finite_sets, each = 5000), dist_values = c( purrr::rdunif(5000, 1 , 6), purrr::rdunif(5000, 1, 12), purrr::rdunif(5000, 1, 20) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + scale_fill_discrete(breaks = finite_sets) + ggtitle(&quot;Histogram of uniform distributions for various inputs&quot;) R Practice We finish with a comprehensive example of an univariate discrete distribution question in R. For inspiration consider the BBC’s cookbook that highlights how to create their distinctive graphics predominately using ggplot2.↩︎ This probability was chosen based on Table 1 from Sex ratios at birth in the United Kingdom, 2014-18 from the Department of Health &amp; Social Care.↩︎ This turns out to be the number of live births in Guildford in 2019, as per Table 3 of the ONS’ Birth Summary Tables, England and Wales, 2019. Source: Office for National Statistics under the Open Government Licence.↩︎ purrr is a part of the tidyverse suite of packages↩︎ "],["continuous-probability-distributions.html", "Chapter 3 Continuous Probability Distributions Learning Objectives Theory 3.1 In-built probability distributions Continuous probability distributions covered 3.2 Normal distribution 3.3 Lognormal distribution 3.4 Exponential distribution 3.5 Gamma distribution 3.6 \\(\\chi^2\\) distribution 3.7 Student’s \\(t\\) distribution 3.8 \\(F\\) distribution 3.9 Beta distribution 3.10 Uniform distribution 3.11 Inverse transform method R Practice", " Chapter 3 Continuous Probability Distributions Learning Objectives Define and explain the key characteristics of the continuous distributions: normal, lognormal, exponential, gamma, chi-square, \\(t\\), \\(F\\), beta and uniform on an interval. Evaluate probabilities and quantiles associated with such distributions. Generate discrete and continuous random variables using the inverse transform method. Theory A reminder: R was designed to be used for statistical computing - so it handles randomness well! Using R we can guarantee reproducibility (and enhance sharability) by using the function set.seed(seed) where seed is a single value integer. Using this approach we guarantee the generation of the same sequence of random numbers everytime we call this function. Use ?set.seed to learn more about this function. 3.1 In-built probability distributions A recap: R has in-built functions for probability distributions: d&lt;distribution-name&gt; \\(:=\\) density (“PDF”), i.e. \\(f_X(x)\\) p&lt;distribution-name&gt; \\(:=\\) probability distribution cumulative function (“CDF”), i.e. \\(F_X(x) =\\boldsymbol{P}(X \\leq x)\\) q&lt;distribution-name&gt; \\(:=\\) quantile function, i.e. return \\(x\\) such that \\(\\boldsymbol{P}(X \\leq x) = p\\) r&lt;distribution-name&gt; \\(:=\\) random deviates, i.e. (psuedo) random number generator for a given distribution Where &lt;distribution-name&gt; \\(=\\) Normal, uniform, lognormal, Student’s \\(t\\), Poisson, binormal, Weibull … see ?distributions() for more information To give some quick examples (we will further explore these in more detail later in this chapter): R Code Definition rnorm(1) Generates \\(x_1\\) where \\(X \\sim \\mathcal{N}(0,\\,1)\\) rnorm(y, mean=10, sd=2) Generates \\(\\{y_1,\\,y_2,\\,\\dots\\}\\) with \\(Y \\sim \\mathcal{N}(10,\\,2^2)\\) runif(3, min=5, max=10) Generates \\(\\{z_1,\\,z_2,\\,z_3\\}\\) where \\(Z \\sim \\mathcal{U}(5,\\,10)\\) dbinom(4, size=5, prob=0.5) Computes \\(\\boldsymbol{P}(X = 4)\\) where \\(X \\sim Bin(5,\\,0.5)\\) pgamma(0.2, shape=2, rate=2) Computes \\(F_Y(0.2)\\) where \\(Y \\sim \\mathcal{\\Gamma}(2,\\,2)\\), i.e. \\(\\boldsymbol{P}(Y\\leq 0.2)\\) qexp(0.5, rate = 2) Determines smallest value of \\(z\\) for \\(\\boldsymbol{P}(Z \\leq z) = 0.5\\) where \\(Z \\sim Exp(2)\\) Continuous probability distributions covered We will consider how to interact with the following continuous probability distributions in R: Normal Lognormal Exponential Gamma \\(\\chi^2\\) Student’s \\(t\\) \\(F\\) Beta Uniform (on an interval) For each distribution above we will determine how to calculate: A random deviate following the discrete distribution \\(X\\), The probability density function (“PDF”), \\(P(k_1 \\leq X \\leq k_2)\\) for distribution \\(X\\) over the range \\([k_1,\\,k_2]\\), The cumulative distribution function (“CDF”), \\(P(X \\leq k)\\), and The quantile function to find \\(k\\) representing the value such that \\(P(X \\leq k) = p\\), i.e. the pth percentile. We will finish off with a plot of the distribution. 3.2 Normal distribution We start with generating random deviates from the Normal distribution. If we are interested in the standard normal, R helpfully has default argument values for \\(\\mu = 0\\) and \\(\\sigma = 0\\) so the function call is very concise: # Generate random deviates set.seed(42) rnorm(5) ## [1] 1.3709584 -0.5646982 0.3631284 0.6328626 0.4042683 We can also specify our own values of \\(\\mu\\) and \\(\\sigma\\). With R we need to remember that the \\(\\sigma\\) argument corresponds to the standard deviation, not the variance. # Generate random deviates set.seed(42) rnorm(5, 10, 2) ## [1] 12.741917 8.870604 10.726257 11.265725 10.808537 We next look at the cumulative distribution function for a Normal distribution. In R we can calculate this using pnorm(q, mean, sd) where: q is the quantile of interest, mean is the mean, and sd is standard deviation. # Calculate P(X &lt;= 2) for X~N(0,1) Next we want to find the xth percentile of \\(X \\sim N(\\mu, \\sigma)\\). We use the quantile function, qnorm(mu, sigma). # Find the 99th percentile for X~N(10,2) percentile_99 &lt;- qnorm(0.99, 10, 2) paste0( &quot;The 99th percentile of X~N(10, 2) is &quot;, format(percentile_99, digits = 4), &quot;.&quot; ) ## [1] &quot;The 99th percentile of X~N(10, 2) is 14.65.&quot; As is customary we finish with a plot of the normal distribution. # Plot the distribution function of X~Normal(mu =, sigma = ) 3.3 Lognormal distribution # Plot the distribution function of Y~Lognormal() 3.4 Exponential distribution # Plot the distribution function of Z~Exp() 3.5 Gamma distribution # Plot the distribution of X~Gamma() 3.6 \\(\\chi^2\\) distribution # Plot the distribution of Y~Chi-Square() 3.7 Student’s \\(t\\) distribution # Plot the distribution of Z~t() 3.8 \\(F\\) distribution # Plot the distribution of X~F() 3.9 Beta distribution # Plot the distribution of Y~Beta() 3.10 Uniform distribution # Plot the distribution of Z~Uniform() 3.11 Inverse transform method The inverse transform method is a way to generate psuedo-random numbers from any probability distribution. One possible algorithm is as follows: Generate a random number \\(u\\) from $U (0, 1) Find the inverse of the desired cumulative distribution function, \\(F^{-1}_X(x)\\) Compute \\(X = F^{-1}_X(u)\\) Suppose we wanted to draw 10,000 random numbers from \\(X \\sim Exp(\\lambda = 2)\\). In order to use the inverse transform method we first need to find the inverse of the CDF. For any \\(X \\sim Exp(\\lambda)\\), the inverse of the CDF is \\(\\frac{-log(1-x)}{\\lambda}\\). We can thus use the inverse transform algorithm to generate random deviates following \\(X \\sim Exp(2)\\): # Step 0 - to guarantee reproducibility set.seed(42) # Step 1 - generate 10,000 random deviates from U[0,1] u &lt;- runif(10000) # Step 2 - find the inverse of the CDF: 1 - exp(-lambda.x) # Inverse of CDF = -log(1 - x) / lambda # Step 3 - compute X using the inverse of the CDF [from step 2] and the random deviates u [from step 1] x &lt;- -log(1 - u) / 2 # Plot the resulting x deviates library(ggplot2) df &lt;- data.frame(x = x) ggplot(df, aes(x=x)) + geom_histogram(binwidth = 0.5, colour=&quot;black&quot;, fill=&quot;white&quot;) R Practice We finish with a comprehensive example of an univariate continuous distribution question in R. "],["joint-and-conditional-distributions.html", "Chapter 4 Joint and Conditional Distributions Learning Objectives Theory R Practice", " Chapter 4 Joint and Conditional Distributions Learning Objectives Explain what is meant by jointly distributed random variables, marginal distributions and conditional distributions. Define the probability function/density function of a marginal distribution and of a conditional distribution. Specify the conditions under which random variables are independent. Define the expected value of a function of two jointly distributed random variables, the covariance and correlation coefficient between two variables, and calculate such quantities. Define the probability function/density function of the sum of two independent random variables as the convolution of two functions. Derive the mean and variance of linear combinations of random variables. Theory Joint distributions are a fundamental concept in actuarial statistics, extending the study of single random variables to situations involving two or more random variables simultaneously. This area of study is covered in Chapter 4 of the CS1 course materials. Here’s a summary of key aspects related to joint distributions: 4.0.1 1. Definition and Representation A joint distribution describes the probability of two or more random variables taking on specific values or falling within certain ranges. Joint Probability Function (Discrete): For discrete random variables X and Y, the joint probability function, P(X = x, Y = y), must satisfy two conditions: P(X = x, Y = y) ≥ 0 for all possible values of x and y. Σx Σy P(X = x, Y = y) = 1 over all possible pairs of x and y. Joint Probability Density Function (Continuous): For continuous random variables X and Y, the joint probability density function, f(x, y), must satisfy: f(x, y) ≥ 0 for all x and y. ∫∫ f(x, y) dy dx = 1 over the entire range of X and Y. This function can be used to calculate probabilities over a specified area, P(x1 &lt; X &lt; x2, y1 &lt; Y &lt; y2) = ∫(x1 to x2) ∫(y1 to y2) f(x, y) dy dx. 4.0.2 2. Marginal and Conditional Distributions From a joint distribution, you can derive marginal distributions (for individual variables) and conditional distributions (for one variable given the value of another). Marginal Probability (Density) Functions: These functions describe the probability distribution of each random variable independently, derived by summing or integrating the joint function over the other variable’s entire range: For discrete variables: P(X = x) = Σy P(X = x, Y = y). For continuous variables: fX(x) = ∫ f(x, y) dy and fY(y) = ∫ f(x, y) dx. Conditional Probability (Density) Functions: These functions describe the distribution of one variable given that the other variable has taken a specific value: P(X = x | Y = y) = P(X = x, Y = y) / P(Y = y) (for discrete cases). fX|Y(x | y) = f(x, y) / fY(y) (for continuous cases). 4.0.3 3. Independence of Random Variables Two random variables, X and Y, are considered independent if the joint probability (density) function can be expressed as the product of their marginal probability (density) functions: * P(X = x, Y = y) = P(X = x) P(Y = y) (discrete). * f(x, y) = fX(x) fY(y) (continuous). If variables are independent, information about one does not change the probability distribution of the other. 4.0.4 4. Expectation, Covariance, and Correlation These measures quantify aspects of the joint behaviour of random variables. Expectation of a Function: The expected value of a function g(X, Y) of two random variables is calculated by summing/integrating g(x, y) multiplied by the joint probability (density) function: E[g(X, Y)] = Σx Σy g(x, y) P(X = x, Y = y) (discrete). E[g(X, Y)] = ∫∫ g(x, y) f(x, y) dy dx (continuous). Covariance (cov[X, Y]): This measures the linear relationship between two random variables. cov[X, Y] = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]. If X and Y are independent, their covariance is zero. Correlation Coefficient (ρ or corr(X, Y)): Pearson’s linear correlation coefficient is a dimensionless measure of the strength and direction of a linear relationship. corr(X, Y) = cov[X, Y] / (sqrt(var[X] var[Y])). The value of corr(X, Y) ranges from -1 to +1. A value of 1 or -1 indicates a perfect linear relationship, while 0 indicates no linear relationship. 4.0.5 5. Linear Combinations and Sums of Random Variables Understanding the distribution of sums or linear combinations of random variables is crucial in actuarial science. Mean of Linear Combinations: The expectation of a sum or linear combination is the sum of the expectations of individual terms: E[c1X1 + ... + cnXn] = c1E[X1] + ... + cnE[Xn]. Variance of Linear Combinations: The variance of a sum includes covariance terms: var[c1X1 + ... + cnXn] = Σ(i=1 to n) c_i^2 var[Xi] + 2 Σ(i&lt;j) ci cj cov[Xi, Xj]. If the random variables are uncorrelated (or independent), the covariance terms are zero, simplifying the formula to var[Σ c_i X_i] = Σ c_i^2 var[X_i]. Moment Generating Functions (MGFs) for Sums: MGFs are powerful tools for determining the distribution of sums of independent random variables. If Y = X1 + ... + Xn and X1, ..., Xn are independent, MY(t) = MX1(t) * ... * MXn(t). If they are also identically distributed (IID), MY(t) = [MX(t)]^n. Examples include the sum of independent Poisson variables being Poisson, and the sum of independent Exponential variables being Gamma. Convolutions: These are used to find the probability function or density function of a sum of independent random variables. 4.0.6 6. Practical Applications The principles of joint distributions are applied in various problem types within the CS1 curriculum, including calculations of probabilities, means, variances, covariances, and correlations for discrete and continuous scenarios. They are foundational for understanding concepts in later chapters, such as conditional expectation and the Central Limit Theorem. R Practice "],["central-limit-theorem.html", "Chapter 5 Central Limit Theorem Learning Objectives Theory R Practice", " Chapter 5 Central Limit Theorem Learning Objectives State the Central Limit Theorem for a sequence of independent, identically distributed random variables. Generate simulated samples from a given distribution and compare the sampling distribution with the Normal. Theory 5.0.0.1 1. Statement of the Central Limit Theorem (CLT) for a sequence of independent, identically distributed random variables The Central Limit Theorem (CLT) is a powerful statistical theorem that describes the shape of the sampling distribution of the sample mean (or sum) from any population, provided the sample size is sufficiently large. For the Sample Mean (X̄): If you draw a random sample of size n from any population with a finite mean (μ) and finite variance (σ²), then the distribution of the sample mean (X̄) will approximately follow a Normal distribution for large n. Specifically, the standardised sample mean, given by (X̄ - μ) / (σ/√n), approaches a standard normal distribution, N(0,1), as n tends to infinity. This implies that for a sufficiently large n, X̄ ≈ N(μ, σ²/n). Crucial Note: This approximation becomes exact for any sample size n if the parent population itself is already normally distributed. For the Sum of Sample Values (ΣXᵢ): Similarly, for the sum of n independent and identically distributed (IID) random variables (ΣXᵢ) from a population with mean (μ) and variance (σ²), the distribution of this sum will approximately follow a Normal distribution for large n. The standardised sum, given by (ΣXᵢ - nμ) / (σ√n), approaches a standard normal distribution, N(0,1), as n tends to infinity. This means that for large enough n, ΣXᵢ ≈ N(nμ, nσ²), which is exactly true if the original population is normal. Factor to Consider: The “degree of skewness” of the underlying population is a key factor in determining the minimum sample size needed for the CLT to provide a reasonable approximation. 5.0.0.2 2. Generating Simulated Samples and Comparing with the Normal Distribution While specific R code for visual comparison isn’t extensively detailed across all provided sources for this exact learning objective, the principle is a critical part of understanding the CLT. Simulation Process: Choose a non-normal distribution (e.g., Exponential, Poisson, etc.) with known parameters. Repeatedly draw a large number (e.g., 1,000 or 10,000) of random samples of a specific size (n) from this chosen distribution. You can use R functions like rpois(), rexp(), etc., for this. For each of these samples, calculate the sample mean (or sum). This collection of sample means forms your empirical “sampling distribution”. Comparison with the Normal Distribution: Plot a histogram of your empirically generated sampling distribution (of sample means). Calculate the theoretical mean (μ) and variance (σ²/n) for the Normal distribution that the CLT predicts for your sample means. Overlay the probability density function (PDF) of this theoretical Normal distribution onto the histogram of your simulated sample means. Observe that as the sample size (n) used in each simulation increases, the histogram of the sampling distribution will increasingly resemble the bell-shaped, symmetrical Normal distribution, regardless of the initial non-normal shape of the parent population. This visual demonstration powerfully illustrates the CLT in action. R Practice "],["data-analysis.html", "Chapter 6 Data Analysis Learning Objectives Theory R Practice", " Chapter 6 Data Analysis Learning Objectives Describe the possible aims of a data analysis (e.g. descriptive, inferential, and predictive). Describe the stages of conducting a data analysis to solve real-world problems in a scientific manner and describe tools suitable for each stage. Describe sources of data and explain the characteristics of different data sources, including extremely large data sets. Explain the meaning and value of reproducible research and describe the elements required to ensure a data analysis is reproducible. Theory 6.0.1 1. Definition and Purpose Data analysis is the process of gathering raw data and then analysing or processing it into information that can be used for specific purposes. The overall basis of this course involves dealing with data, such as claim types and amounts, and summarising and analysing them to construct statistical models for predictions. 6.0.1.1 2. Key Forms of Data Analysis Data analysis primarily takes three forms: * Descriptive Analysis: Involves summarising data into a simpler, more easily understood format using summary statistics (e.g., measures of central tendency like mean, median, mode; and measures of dispersion like standard deviation, interquartile range, skewness) and graphical displays. It aims to describe the data presented, not to draw specific conclusions. * Inferential Analysis: Draws conclusions about a population based on data gathered from a sample. It involves estimating population parameters and testing hypotheses, often assuming large, randomly selected samples accurately represent the wider population. * Predictive Analysis: Extends inferential analysis principles to analyse past data and make predictions about future events. This often involves using a “training set” to identify relationships and a “test set” to assess their strength, with regression analysis being a typical example. 6.0.1.2 3. The Data Analysis Process The key steps in a data analysis process are sequential and systematic: 1. Develop clear objectives for the analysis. 2. Identify required data items. 3. Collect data from appropriate sources. 4. Process and format data (e.g., inputting into spreadsheets or databases). 5. Clean data by addressing unusual, missing, or inconsistent values. 6. Conduct exploratory data analysis (which may include descriptive, inferential, or predictive analysis). 7. Model the data. 8. Communicate the results. 9. Monitor the process by updating data and repeating analysis if necessary. 6.0.1.3 4. Types of Data Data can be categorised to define how it should be summarised and analysed: * Numerical (Quantitative) Data: Consists of numbers or quantities. * Discrete Data: Can only take particular whole values (e.g., number of claims). Typically obtained by counting. * Continuous Data: Can take any value within a specified range (e.g., claim amount, time). Typically obtained by measuring and often rounded. * Categorical (Qualitative) Data: Consists of non-numerical information. * Attribute (Dichotomous) Data: Has only two categories (e.g., claim/no claim). * Nominal Data: Cannot be ordered in any way (e.g., hair colour, policy type). * Ordinal Data: Can be ordered (e.g., tidiness ratings, agreement levels). 6.0.1.4 5. Summarising Data In Tables: Frequency Distributions: Show how frequencies are shared amongst data values. Suitable for categorical or discrete numerical data. Grouped Frequency Distributions: Organise data into classes or groups, suitable for continuous data where values are spread out. Cumulative Frequency Tables: Accumulate (add up) frequencies, useful for finding positions of data values like the median. In Diagrams: Bar Charts: Used for discrete or categorical data, displaying frequency with bars that have gaps between them. Histograms: Used for continuous data, similar to bar charts but with a continuous x-axis and no gaps between bars. The area of each bar represents the frequency, requiring calculation of frequency density (frequency / class width) for bar height. Stem and Leaf Diagrams: Split each data value into a “stem” (e.g., tens) and a “leaf” (e.g., units). Useful for showing data distribution and finding quartiles. Cumulative Frequency Graphs: Plot the cumulative frequency against the upper class boundary of each group, useful for estimating percentiles. Boxplots (Box and Whisker Plots): Display the lowest value, highest value, median, lower quartile (Q1), and upper quartile (Q3). They effectively show the middle 50% of data and are useful for comparing data sets. 6.0.1.5 6. Comparing Data Sets When comparing data sets, focus on three key features: * Location: Where the data is centred or grouped (e.g., using mean, median, mode). * Spread: How spread out or variable the values are (e.g., using range, interquartile range, standard deviation). * Skewness (Shape): The asymmetry of the distribution (positively skewed, symmetrical, or negatively skewed). 6.0.1.6 7. Data Management Principles Data Sources: Primary source is the original population or sample. A secondary source makes this collected information available for others. Data Characteristics: Censored Data: The value of a variable is only partially known (e.g., a lower bound in survival studies). Truncated Data: Measurements are completely unknown as they were not recorded. Big Data: Characterised by: Volume: Size of the data set. Velocity: Speed of data arrival and processing. Variety: Data from many different sources and formats. Veracity/Reliability: Difficulty in ascertaining reliability of individual data elements. 6.0.1.7 8. Reproducibility Replication refers to an independent third party repeating an experiment and obtaining consistent results. It can be difficult if studies are large, rely on expensive data, or involve unique occurrences. Reproducibility refers to ensuring an analysis can be precisely re-run, yielding the same results. This is often taken as a reasonable alternative standard to replication. Elements required for reproducibility include original data, fully documented computer code, good version control, details of the software environment, and setting random seeds where randomness is involved. Value of Reproducibility: Essential for technical review, regulatory compliance, extending research, comparison, and error reduction in actuarial analysis. R Practice "],["exploratory-data-analysis.html", "Chapter 7 Exploratory Data Analysis Learning Objectives Theory R Practice", " Chapter 7 Exploratory Data Analysis Learning Objectives Describe the purpose of exploratory data analysis. Use appropriate tools to calculate suitable summary statistics and undertake exploratory data visualizations. Define and calculate Pearson’s, Spearman’s and Kendall’s measures of correlation for bivariate data, explain their interpretation and perform statistical inference as appropriate. Use Principal Components Analysis to reduce the dimensionality of a complex data set. Theory 7.0.0.1 1. Purpose of Exploratory Data Analysis EDA is the process of analysing data to gain initial insights into its nature, patterns, and relationships between variables before formal statistical techniques are applied. Its primary aim is to understand “what is going on” with the data, summarising it into a more easily understood format. 7.0.0.2 2. Tools for Summary Statistics and Data Visualisation EDA employs various tools depending on the number of variables: For Univariate Data (Single Variable): Summary Statistics: Measures of central tendency: Mean, Median, Mode. Measures of dispersion: Standard Deviation, Interquartile Range (IQR), Range, Skewness. Graphical Displays: Line plots. Bar charts (for discrete or categorical data). Histograms (for continuous data, where bar area represents frequency). Stem and leaf diagrams (displaying distribution and aiding quartile identification). Cumulative frequency graphs (for estimating percentiles). Boxplots (Box and Whisker Plots) (showing lowest/highest values, median, quartiles; useful for comparing datasets). Quantile-quantile (Q-Q) plots. For Bivariate or Multivariate Data (Multiple Variables): Summary Statistics: Individual variable summary statistics. Graphical Displays: Scatterplots (to visualise relationships between pairs of variables). 7.0.0.3 3. Correlation Measures for Bivariate Data These coefficients quantify the strength and direction of relationships between variables: Pearson’s Correlation Coefficient (r): Definition: Measures the strength and direction of a linear relationship between two quantitative variables. Formula: \\(r = \\frac{s_{xy}}{s_x . s_y}\\) Interpretation: Values range from -1 to +1. Near +1 indicates a strong positive linear relationship, near -1 indicates a strong negative linear relationship, and near 0 indicates no linear correlation. Statistical Inference: A t-distribution based test is available for the hypothesis that the population correlation coefficient (ρ) is zero. Fisher’s transformation can be used for testing hypotheses about specific non-zero values of ρ. Crucial Caveat: Correlation does not necessarily imply causation. Spearman’s Rank Correlation Coefficient (r_s): Definition: Measures the strength of a monotonic (not necessarily linear) relationship between two variables. Calculation: Applied by calculating Pearson’s formula on the ranks of the data. Statistical Inference: Tests exist for both small samples (using permutations of ranks) and medium to large samples (using an approximate Normal distribution). Kendall’s Rank Correlation Coefficient (τ): Definition: Another non-parametric measure of monotonic association, based on the number of concordant and discordant pairs in the data. Statistical Inference: Similar to Spearman’s, tests are available for small samples (permutations) and medium to large samples (using an approximate Normal distribution). 7.0.0.4 4. Principal Components Analysis (PCA) Purpose: PCA is a method for reducing the dimensionality of a complex dataset by identifying the key components necessary to model and understand the data. Mechanism: It creates uncorrelated linear combinations of the original variables, where these new components (principal components) maximise the variance explained. Process: Calculate the centred data. Obtain eigenvectors of the scaled covariance matrix, which represent the “rotation” of the data. Compute the principal components, which are the new uncorrelated variables. Evaluate the explanatory power (variance) of each component. Reduce the number of components by discarding those that explain less variance. Reconstruct the original data using the reduced set of components. Component Selection Criteria: Cumulative Variance Explained: Retain components that collectively explain a target percentage (e.g., 90%) of the total variance. Scree Test: Plot a scree diagram and keep components before the graph “levels off”. Kaiser Criterion: If data is scaled, keep components whose variance is greater than 1. R Practice "],["random-sampling.html", "Chapter 8 Random Sampling Learning Objectives Theory R Practice", " Chapter 8 Random Sampling Learning Objectives Explain what is meant by a sample, a population and statistical inference. Define a random sample from a distribution of a random variable. Explain what is meant by a statistic and its sampling distribution. Determine the mean and variance of a sample mean and the mean of a sample variance in terms of the population mean, variance and sample size. State and use the basic sampling distributions for the sample mean and the sample variance for random samples from a normal distribution. State and use the distribution of the \\(t\\)-statistic for random samples from a normal distribution. State and use the \\(F\\) distribution for the ratio of two sample variances from independent samples taken from normal distributions. Theory 8.0.0.1 1. What is Meant by a Sample, a Population, and Statistical Inference? Population: This refers to the entire group of individuals or items that we are interested in studying. For example, all car insurance policies in a company. Sample: This is a subset of the population from which data is collected. It’s a manageable group used to draw conclusions about the larger, often impractical-to-observe, population. Statistical Inference: This is the process of drawing conclusions about a population based on data obtained from a sample. It extends inferential analysis from a sample to the wider population. This involves estimating population parameters and testing hypotheses. 8.0.0.2 2. Defining a Random Sample A random sample is a set of random variables, X₁, X₂, …, Xₙ, that are independent and identically distributed (IID) from a specific population. Characteristics of a random sample: Each item in the population has a probability of inclusion in the sample that is proportional to its frequency in the parent population. The inclusion or exclusion of different items operates independently. 8.0.0.3 3. Understanding a Statistic and its Sampling Distribution Statistic: A statistic is a function derived solely from a random sample X. For instance, the sample mean (X̄ = ΣXᵢ / n) and sample variance (S² = Σ(Xᵢ - X̄)² / (n-1)) are examples of statistics. Sampling Distribution: The distribution of a statistic is known as its sampling distribution. Standard Error: The standard deviation of a sampling statistic is specifically termed its standard error. 8.0.0.4 4. Mean and Variance of Sample Mean (X̄) and Sample Variance (S²) For a random sample of size n from a population with mean μ and variance σ²: Mean of the Sample Mean (E[X̄]): E[X̄] = μ. Derivation: E[X̄] = E[(1/n) ΣXᵢ] = (1/n) ΣE[Xᵢ] = (1/n) Σμ = (1/n) * nμ = μ. Variance of the Sample Mean (Var[X̄]): Var[X̄] = σ²/n. Derivation: Var[X̄] = Var[(1/n) ΣXᵢ] = (1/n)² ΣVar[Xᵢ] (due to independence) = (1/n)² Σσ² = (1/n)² * nσ² = σ²/n. Mean of the Sample Variance (E[S²]): E[S²] = σ². This indicates that the sample variance S² is an unbiased estimator of the population variance σ². Derivation: E[S²] = E[(1/(n-1)) Σ(Xᵢ - X̄)²] = E[(1/(n-1)) (ΣXᵢ² - nX̄²)]. Using E[Xᵢ²] = Var[Xᵢ] + E[Xᵢ]² = σ² + μ² and E[X̄²] = Var[X̄] + E[X̄]² = (σ²/n) + μ², the derivation simplifies to E[S²] = σ². 8.0.0.5 5. Basic Sampling Distributions for Normal Samples Sample Mean (X̄) when population variance (σ²) is known: If a random sample is drawn from a Normal(μ, σ²) distribution, then the sample mean X̄ follows a Normal(μ, σ²/n) distribution exactly. The pivotal quantity (X̄ - μ) / (σ/√n) follows a Standard Normal (N(0,1)) distribution. This also holds approximately for large samples from any distribution due to the Central Limit Theorem. Sample Variance (S²) from a Normal population: If S² is the sample variance from a Normal(μ, σ²) distribution, then (n-1)S²/σ² follows a Chi-squared (χ²) distribution with (n-1) degrees of freedom. Important Note: This result is strictly for samples from normal distributions and does not hold for non-normal distributions. 8.0.0.6 6. The t-statistic for Random Samples from a Normal Distribution Sample Mean (X̄) when population variance (σ²) is unknown: If S is the sample standard deviation from a Normal(μ, σ²) distribution, then the t-statistic (X̄ - μ) / (S/√n) follows a t-distribution with (n-1) degrees of freedom. This distribution is more spread out than the standard normal due to the additional uncertainty from estimating σ² with S. 8.0.0.7 7. F-Distribution for Ratio of Two Sample Variances from Independent Normal Samples Definition of F-distribution: If U and V are independent random variables following Chi-squared (χ²) distributions with m and n degrees of freedom respectively, then the ratio (U/m) / (V/n) follows an F-distribution with m and n degrees of freedom (Fₘ,ₙ). Application for variance ratios: For two independent random samples from Normal(μ₁, σ₁²) and Normal(μ₂, σ₂²) distributions, with sample variances S₁² and S₂² respectively: The ratio (S₁²/σ₁²) / (S₂²/σ₂²) follows an F-distribution with (n₁-1) and (n₂-1) degrees of freedom (F(n₁-1),(n₂-1)). This is typically used to test for the equality of two population variances (i.e., H₀: σ₁² = σ₂²). Keep these critical points at your fingertips, and you’ll be well on your way to acing your CS1 exam! R Practice "],["estimation-and-estimators.html", "Chapter 9 Estimation and estimators Learning Objectives Theory R Practice", " Chapter 9 Estimation and estimators Learning Objectives Describe and apply the method of moments for constructing estimators of population parameters. Describe and apply the method of maximum likelihood for constructing estimators of population parameters. Define the terms: efficiency, bias, consistency and mean squared error. Define and apply the property of unbiasedness of an estimator. Define the mean square error of an estimator, and use it to compare estimators. Describe and apply the asymptotic distribution of maximum likelihood estimators. Use the bootstrap method to estimate properties of an estimator. Theory Point Estimation (Estimation and Estimators) This chapter focuses on different methods for estimating unknown population parameters from sample data and evaluating the quality of those estimators. 9.0.0.1 1. Method of Moments (MOM) for Constructing Estimators Definition: The Method of Moments involves equating sample moments to their corresponding population (or theoretical) moments. Application: For 1 unknown parameter (e.g., Poi(μ), Exp(λ)): Equate the sample mean (x̄) to the population mean (E[X]). Example: For an Exp(λ) distribution, λ̂ = 1/x̄. For 2 unknown parameters (e.g., NBin(k,p), Gamma(α,λ), N(μ,σ²)): Equate the sample mean and sample variance to their population counterparts. Alternatively, equate the first two moments about zero. Characteristics: May be simple to calculate. May sometimes produce inadmissible values (estimates that are not possible within the parameter space). 9.0.0.2 2. Method of Maximum Likelihood (MLE) for Constructing Estimators Definition: The Maximum Likelihood Estimate (MLE) is the value of the parameter that maximizes the likelihood of observing the given sample data. In essence, it finds the parameter value that makes the observed data “most probable”. Application Stages (Single-parameter case): Write down the likelihood function (L): This is the probability (for discrete data) or probability density (for continuous data) of obtaining the observed sample, viewed as a function of the parameter(s). Obtain the log-likelihood (ln L): Taking the natural logarithm simplifies differentiation. Differentiate and set to zero: Differentiate ln L with respect to the parameter(s) and set the derivative(s) equal to zero. Solving these equations yields the MLE(s). (Optional check): Differentiate again to ensure the second derivative is negative, confirming a maximum. Examples: For a Poisson(μ) distribution, the MLE of μ is the sample mean (x̄). Tricky Situations: Incomplete data: For censored or truncated data, the method requires careful formulation of the likelihood function. Domain parameters: When the parameter’s possible values are bounded (e.g., uniform distribution’s upper bound), the MLE might be found at the boundary of the parameter space, often requiring visual inspection or numerical methods rather than differentiation. 9.0.0.3 3. Definitions: Efficiency, Bias, Consistency, Mean Squared Error Bias: For an estimator θ̂ of a parameter θ, the bias is defined as Bias(θ̂) = E[θ̂] - θ. Mean Squared Error (MSE): For an estimator θ̂ of a parameter θ, the MSE is defined as MSE(θ̂) = E[(θ̂ - θ)²]. Efficiency: An estimator θ̂₁ is considered more efficient than another estimator θ̂₂ if MSE(θ̂₁) &lt; MSE(θ̂₂). A lower MSE means the estimator is, on average, closer to the true parameter value. Consistency: An estimator θ̂ is consistent if its MSE(θ̂) → 0 as the sample size n → ∞. This implies that with a sufficiently large sample, the estimator will converge to the true parameter value. 9.0.0.4 4. Property of Unbiasedness of an Estimator Definition: An estimator θ̂ is unbiased for θ if its bias is zero, i.e., E[θ̂] = θ. This means, on average, the estimator will hit the true parameter value. Application: The sample mean (x̄) is an unbiased estimator of the population mean (μ). The sample variance (S²) calculated with 1/(n-1) in the denominator is an unbiased estimator of the population variance (σ²). 9.0.0.5 5. Mean Squared Error (MSE) and Comparison of Estimators Formula: The MSE can be decomposed into the variance of the estimator and the square of its bias: MSE(θ̂) = Var(θ̂) + (Bias(θ̂))². This formula is key for evaluating an estimator’s overall quality. Comparison: To compare estimators, calculate their MSEs. The estimator with the smaller MSE is preferred because it represents a better balance between bias and variance, indicating greater accuracy around the true parameter. Sometimes, a slightly biased estimator might be chosen over an unbiased one if its MSE is significantly lower. 9.0.0.6 6. Asymptotic Distribution of Maximum Likelihood Estimators Asymptotic Property: For large sample sizes, the Maximum Likelihood Estimator (MLE), θ̂, is asymptotically normally distributed: θ̂ ~ N(θ, CRLB). Approximate Standard Error: For a sufficiently large sample size, the standard error of the MLE is approximately equal to the square root of the Cramer-Rao Lower Bound (CRLB). Cramer-Rao Lower Bound (CRLB): This is the theoretical minimum variance an unbiased estimator can achieve. It’s given by CRLB = 1 / E[-(d²lnL/dθ²)]. For instance, for the Poisson(μ) parameter, the CRLB is μ/n. If an unbiased estimator’s variance equals the CRLB, it is considered a minimum variance unbiased estimator (MVUE). 9.0.0.7 7. Bootstrap Method to Estimate Properties of an Estimator Purpose: The bootstrap method is a resampling technique used to estimate the sampling distribution of a statistic, which in turn helps in constructing confidence intervals or estimating other properties of an estimator when analytical methods are difficult or impossible. Method Outline: Generate Resamples: Create multiple “bootstrap samples” by drawing observations with replacement from the original observed sample (non-parametric bootstrap). Alternatively, if a distribution is assumed, simulate samples from the fitted distribution (parametric bootstrap). Calculate Statistic: For each of these bootstrap samples, calculate the statistic of interest (e.g., the estimator of the parameter). Empirical Distribution: The collection of calculated statistics from all bootstrap samples forms an empirical sampling distribution. This distribution can then be used for statistical inference, such as deriving confidence intervals by finding appropriate quantiles of the bootstrap distribution. R Practice "],["confidence-intervals.html", "Chapter 10 Confidence Intervals Learning Objectives Theory R Practice", " Chapter 10 Confidence Intervals Learning Objectives Define in general terms a confidence interval for an unknown parameter of a distribution based on a random sample. Derive a confidence interval for an unknown parameter using a given sampling distribution. Calculate confidence intervals for the mean and the variance of a normal distribution. Calculate confidence intervals for a binomial probability and a Poisson mean, including the use of the normal approximation in both cases. Calculate confidence intervals for two-sample situations involving the normal distribution, and the binomial and Poisson distributions using the normal approximation. Calculate confidence intervals for a difference between two means from paired data. Use the bootstrap method to obtain confidence intervals. Theory Confidence Intervals This chapter focuses on constructing intervals that, with a specified level of confidence, contain the true (but unknown) value of a population parameter. 10.0.0.1 1. Define in general terms a confidence interval for an unknown parameter of a distribution based on a random sample. Definition: A confidence interval (CI) for a population parameter \\(\\theta\\) is an interval (A, B) whose bounds (A and B) are random variables. Purpose: This interval is constructed such that it contains the fixed but unknown value of the parameter \\(\\theta\\) with a specified probability, for example, 95%. Notation: P(A &lt; θ &lt; B) = 95%. Key Distinction: It is crucial to differentiate a confidence interval from a prediction interval. A CI estimates a population parameter (\\(\\theta\\)), while a prediction interval estimates a future observed value (X_(n+1)) from the population. 10.0.0.2 2. Derive a confidence interval for an unknown parameter using a given sampling distribution. The general approach to deriving confidence intervals is the Pivotal Method: 1. Identify a Pivotal Quantity: Find a formula (a “pivotal quantity”) that involves the unknown parameter \\(\\theta\\) (or the future random variable \\(X_{n+1}\\)) and has a known sampling distribution. 2. Determine Critical Values: Use the percentage point tables of the known distribution to find the appropriate critical values corresponding to the desired confidence level (e.g., for a 95% symmetrical CI, you’d find the 2.5th and 97.5th percentiles). 3. Construct and Rearrange: Set up an inequality using the pivotal quantity and the critical values, then algebraically rearrange this inequality to isolate the unknown parameter \\(\\theta\\) in the middle. The resulting bounds will form the confidence interval. 10.0.0.3 3. Calculate confidence intervals for the mean and the variance of a normal distribution. For the Mean (\\(\\mu\\)) of a Normal Population: Variance (\\(\\sigma^2\\)) Known: Pivotal Quantity: \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\). This is exact for normal populations and approximate for large samples from any distribution (due to the CLT). Example calculation for 95% CI: \\(\\bar{x} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\\). Variance (\\(\\sigma^2\\)) Unknown: Pivotal Quantity: \\(\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}\\). This is given on page 22 of the Tables. Example calculation for 95% CI: \\(\\bar{x} \\pm t_{n-1, 0.025} \\frac{S}{\\sqrt{n}}\\). For the Variance (\\(\\sigma^2\\)) of a Normal Population: Pivotal Quantity: \\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\). This result only holds for samples from a normal distribution. This is given on page 22 of the Tables. Example calculation for 95% CI for \\(\\sigma\\): \\(\\sqrt{\\left(\\frac{(n-1)S^2}{\\chi^2_{n-1, 0.025}}\\right)}, \\sqrt{\\left(\\frac{(n-1)S^2}{\\chi^2_{n-1, 0.975}}\\right)}\\). 10.0.0.4 4. Calculate confidence intervals for a binomial probability and a Poisson mean, including the use of the normal approximation in both cases. For a Binomial Probability (\\(p\\)): Exact: The exact distribution is \\(X \\sim Bin(n,p)\\). Normal Approximation (for large \\(n\\)): \\(X \\sim N(np, npq)\\). Pivotal Quantity: \\(\\frac{\\hat{p} - p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}} \\sim N(0,1)\\). The approximation is reasonable if \\(np &gt; 5\\) and \\(n(1-p) &gt; 5\\). Example calculation for 95% CI: \\(\\hat{p} \\pm 1.96 \\sqrt{\\hat{p}(1-\\hat{p})/n}\\). For a Poisson Mean (\\(\\lambda\\)): Exact: The exact distribution of the sum of observations is \\(X \\sim Poi(n\\lambda)\\). Normal Approximation (for large \\(n\\)): \\(\\sum X_i \\sim N(n\\lambda, n\\lambda)\\). Pivotal Quantity: \\(\\frac{\\bar{X} - \\lambda}{\\sqrt{\\bar{X}/n}} \\sim N(0,1)\\). The approximation is reasonable if \\(\\lambda\\) is large, typically \\(\\lambda &gt; 5\\). 10.0.0.5 5. Calculate confidence intervals for two-sample situations involving the normal distribution, and the binomial and Poisson distributions using the normal approximation. Normal Distribution (Difference in Means, \\(\\mu_1 - \\mu_2\\)): Known Variances (\\(\\sigma_1^2, \\sigma_2^2\\)): Pivotal Quantity: \\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1 + \\sigma_2^2/n_2}} \\sim N(0,1)\\). Exact for independent normal samples, approximate for large independent samples. Unknown but Equal Variances (\\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\)): Pivotal Quantity: \\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{S_p \\sqrt{1/n_1 + 1/n_2}} \\sim t_{n_1+n_2-2}\\). \\(S_p^2\\) is the pooled variance. This is given on page 23 of the Tables. Normal Distribution (Ratio of Variances, \\(\\sigma_1^2 / \\sigma_2^2\\)): Pivotal Quantity: \\(\\frac{S_1^2/\\sigma_1^2}{S_2^2/\\sigma_2^2} \\sim F_{n_1-1, n_2-1}\\). This result only holds for samples from independent normal distributions. Given on page 22 of the Tables. Lower percentage points are found using the relationship \\(F_{n,m, \\alpha} = 1/F_{m,n, 1-\\alpha}\\). Binomial Distributions (Difference in Proportions, \\(p_1 - p_2\\)): Normal Approximation (for large \\(n_1, n_2\\)): Pivotal Quantity: \\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\hat{p}_1(1-\\hat{p}_1)/n_1 + \\hat{p}_2(1-\\hat{p}_2)/n_2}} \\sim N(0,1)\\). Poisson Distributions (Difference in Means, \\(\\lambda_1 - \\lambda_2\\)): Normal Approximation (for large \\(n_1, n_2\\)): Pivotal Quantity: \\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\lambda_1 - \\lambda_2)}{\\sqrt{\\bar{X}_1/n_1 + \\bar{X}_2/n_2}} \\sim N(0,1)\\). 10.0.0.6 6. Calculate confidence intervals for a difference between two means from paired data. Method: Calculate the differences (d) between corresponding data points for each pair. This transforms the two-sample paired problem into a one-sample problem with these differences. Treat these differences (\\(d_1, d_2, \\dots, d_n\\)) as a single random sample from a normal distribution. Construct a one-sample confidence interval for the mean difference (\\(\\mu_D\\)) using the t-distribution: \\(\\frac{\\bar{D} - \\mu_D}{S_D/\\sqrt{n}} \\sim t_{n-1}\\). 10.0.0.7 7. Use the bootstrap method to obtain confidence intervals. Purpose: The bootstrap is a resampling technique used to estimate the sampling distribution of a statistic when analytical methods are difficult or impossible. Method Outline (for CIs): Resampling: Create a large number (e.g., 1,000) of “bootstrap samples” by drawing observations with replacement from the original observed sample (non-parametric bootstrap). Alternatively, if a distribution is assumed, simulate samples from the fitted distribution (parametric bootstrap). Estimate Statistic: For each bootstrap sample, calculate the statistic of interest (e.g., the estimator of the parameter, or the bounds of a confidence interval). Empirical Distribution: The collection of these calculated statistics forms an empirical sampling distribution. Confidence Interval Construction: The desired confidence interval can then be derived by finding the appropriate quantiles of this empirical distribution (e.g., for a 95% CI, use the 2.5th and 97.5th percentiles of the bootstrap distribution). Relevance to CS1: The syllabus objective 3.1.7 specifically mentions using the bootstrap method to estimate properties of an estimator, and it’s also noted for obtaining confidence intervals. R Practice "],["hypothesis-testing.html", "Chapter 11 Hypothesis Testing Learning Objectives Theory R Practice", " Chapter 11 Hypothesis Testing Learning Objectives Explain what is meant by the terms null and alternative hypotheses, simple and composite hypotheses, type I and type II errors, test statistic, likelihood ratio, critical region, level of significance, probability-value and power of a test. Apply basic tests for the one-sample and two-sample situations involving the normal, binomial and Poisson distributions, and apply basic tests for paired data. Apply the permutation approach to non-parametric hypothesis tests. Theory Hypothesis Testing This chapter is designed to equip you with the essential statistical inference tools to draw conclusions about populations from sample data. Mastering these concepts is vital for your CS1 examination and future actuarial practice. 11.0.0.1 Learning Objective 1: Core Concepts of Hypothesis Testing To begin, you must clearly understand the terminology that underpins all hypothesis tests: Null Hypothesis (H₀): This completely specifies the underlying distribution, typically stating an exact value for the unknown parameter under investigation. For instance, H₀: μ = 20 [10.1, 461]. Alternative Hypothesis (H₁): This hypothesis does not completely specify the underlying distribution. It usually defines a range of values for the parameter, such as μ ≠ 20, μ &gt; 20, or μ &lt; 20 [10.1, 461, 462]. Simple Hypothesis: A hypothesis that completely specifies the underlying distribution, often taking the form θ = θ₀ [10.1, 461]. Composite Hypothesis: A hypothesis that does not completely specify the underlying distribution, often taking the form θ &gt; θ₀, θ &lt; θ₀, or θ ≠ θ₀ [10.1, 461]. Type I Error (α): This occurs when you reject a true null hypothesis [4, 10.3, 463]. In binary classification, it’s a “false positive,” where a healthy individual tests positive [10.4, 464]. Its probability is the level of significance of the test [10.3, 463]. Type II Error (β): This occurs when you fail to reject a false null hypothesis [4, 10.3, 463]. In binary classification, it’s a “false negative,” where a diseased individual tests negative [10.4, 464]. Test Statistic: This is a function of a random sample used to make decisions about the null hypothesis. Its observed value is calculated under the assumption that H₀ is true [10.8, 468]. Likelihood Ratio: While listed as a syllabus objective (Syllabus objective 3.3.1), its specific definition is not detailed in the provided sources. Critical Region: This is the range of values for the test statistic that leads to the rejection of the null hypothesis. It is determined by the critical value(s) and the chosen level of significance [10.8, 468]. Level of Significance (α): This is the probability of making a Type I error [4, 10.3, 463]. It defines the threshold for rejecting H₀. Probability-value (p-value): This is the probability of observing a test statistic as extreme as, or more extreme than, the one obtained from the sample data, assuming the null hypothesis is true [4, 10.7, 467]. Significance: A smaller p-value provides stronger evidence against the null hypothesis. If p-value &lt; α (level of significance), you reject H₀ [10.7, 468]. Power of a Test: This is the probability of correctly rejecting a false null hypothesis [10.6, 466]. It is calculated as 1 – β (1 minus the probability of a Type II error) [10.6, 467]. 11.0.0.2 Learning Objective 2: Basic Tests for One-Sample and Two-Sample Situations Applying these tests effectively is a core CS1 skill. Here’s a structured approach for various distributions and scenarios: General Steps for Hypothesis Testing [10.8, 468]: 1. State Hypotheses: Clearly define H₀ and H₁. 2. Choose Test Statistic: Select the appropriate formula based on the parameter being tested and data characteristics. 3. Calculate Observed Value: Compute the test statistic’s value assuming H₀ is true. 4. Obtain Critical Value(s): Find these from statistical tables corresponding to your chosen α and distribution. 5. Compare and Conclude: Compare the observed value to the critical value(s) and state your decision regarding H₀ and the practical implication. I. One-Sample Tests: Normal Mean (Known Variance, σ²): Test Statistic: \\(Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\) [10.9, 470]. Distribution (under H₀): Approximately \\(N(0,1)\\) for large samples; exactly \\(N(0,1)\\) for normal populations [10.9, 470]. Normal Mean (Unknown Variance, σ²): Test Statistic: \\(T = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}}\\) [10.10, 470]. Distribution (under H₀): \\(t_{n-1}\\) (t-distribution with \\(n-1\\) degrees of freedom) [10.10, 470]. Example: Test μ = 19 for a sample mean of 15.8 with \\(S^2 = 895.243\\) and \\(n=15\\). Observed \\(T = -0.414\\). Compared to \\(t_{14}\\) critical values. At 5% level, insufficient evidence to reject H₀ [10.10a, 471, 472]. Normal Variance (σ²): Test Statistic: \\(\\frac{(n-1)S^2}{\\sigma_0^2}\\) [10.11, 472]. Distribution (under H₀): \\(\\chi^2_{n-1}\\) (Chi-squared distribution with \\(n-1\\) degrees of freedom) [10.11, 472]. Example: Test σ² = 190 for a sample \\(S = 15\\) with \\(n=20\\). Observed statistic = 22.5. Compared to \\(\\chi^2_{19}\\) critical values. At 5% level, insufficient evidence to reject H₀ [10.11a, 473, 474]. Binomial Parameter (Proportion, p): Test Statistic (large \\(n\\)): \\(\\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\\) (often with continuity correction) [10.12, 475]. Distribution (under H₀): Approximately \\(N(0,1)\\) [10.12, 475]. Poisson Parameter (Mean, λ): Test Statistic (large \\(n\\)): \\(\\frac{\\bar{X} - \\lambda_0}{\\sqrt{\\lambda_0/n}}\\) (often with continuity correction) [10.13, 476]. Distribution (under H₀): Approximately \\(N(0,1)\\) [10.13, 476]. Example: Test λ &gt; 50 for 2,710 claims in 50 days. Observed statistic = 4.19. Compared to \\(N(0,1)\\) critical value (1.6449). At 5% level, sufficient evidence to reject H₀ [10.13a, 477, 478]. II. Two-Sample Tests: Difference in Normal Means (Known Variances, σ₁², σ₂²): Test Statistic: \\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1 + \\sigma_2^2/n_2}}\\) [10.14, 478, 479]. Distribution (under H₀): Exactly \\(N(0,1)\\) for normal populations, approximately \\(N(0,1)\\) for large samples [10.14, 479]. Difference in Normal Means (Unknown but Equal Variances): Test Statistic: \\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{S_p\\sqrt{1/n_1 + 1/n_2}}\\), where \\(S_p\\) is the pooled standard deviation [10.15, 480]. Distribution (under H₀): \\(t_{n_1+n_2-2}\\) [10.15, 480]. Example: Test if a treatment improves physical strength (μ_A &gt; μ_B) with pooled variance. Observed statistic = 1.32. Compared to \\(t_{14}\\) critical value (1.761). At 5% level, insufficient evidence to reject H₀ [10.15a, 481, 482]. Ratio of Normal Variances (σ₁²/σ₂²): Test Statistic: \\(\\frac{S_1^2/\\sigma_1^2}{S_2^2/\\sigma_2^2}\\) [10.16, 483]. Distribution (under H₀: σ₁² = σ₂²): \\(F_{n_1-1, n_2-1}\\) [10.16, 483]. Example: Test for equal variances. Observed statistic = 0.630. Compared to \\(F_{7,7}\\) critical values (0.200, 4.995). At 5% level, insufficient evidence to reject H₀ [10.16a, 484]. Equality of Binomial Parameters (Proportions, p₁, p₂): Test Statistic (large \\(n_1, n_2\\)): \\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}}\\), where \\(\\hat{p}\\) is the pooled sample proportion [10.17, 485, 486]. Distribution (under H₀: p₁ = p₂): Approximately \\(N(0,1)\\) [10.17, 486]. Example: Test if a product helps students pass exams (p_A &gt; p_B). Observed statistic = 0.852. Compared to \\(N(0,1)\\) critical value (1.6449). At 5% level, insufficient evidence to reject H₀ [10.17a, 486, 487]. Equality of Poisson Parameters (Means, λ₁, λ₂): Test Statistic (large \\(n_1, n_2\\)): \\(\\frac{(\\hat{\\lambda}_1 - \\hat{\\lambda}_2) - (\\lambda_1 - \\lambda_2)}{\\sqrt{\\hat{\\lambda}(1/n_1 + 1/n_2)}}\\), where \\(\\hat{\\lambda}\\) is the pooled sample mean [10.18, 488]. Distribution (under H₀: λ₁ = λ₂): Approximately \\(N(0,1)\\) [10.18, 488]. Paired Data (Mean Difference, μ_D): Approach: Calculate the differences between corresponding data points (\\(D_i = X_i - Y_i\\)) to create a single sample of differences. Then, perform a one-sample t-test on these differences [10.19, 489, 490]. Test Statistic: \\(T = \\frac{\\bar{D} - \\mu_D}{S_D/\\sqrt{n}}\\) [10.19, 489]. Distribution (under H₀: μ_D = 0): \\(t_{n-1}\\) (assuming differences are normally distributed) [10.19, 489]. Example: Test if a treatment increases physical strength for paired data. Observed statistic = 1.73. Compared to \\(t_{5}\\) critical value (2.015). At 5% level, insufficient evidence to reject H₀ [10.19a, 490]. 11.0.0.3 Learning Objective 3: Permutation Approach to Non-Parametric Hypothesis Tests For non-parametric tests, especially when distributional assumptions are not met or sample sizes are small, the permutation approach is key: Permutation Approach: This method involves [10.20, 490, 491]: Calculate Observed Statistic: Compute the test statistic for the original sample data. Generate Permutations: Randomly re-assign (permute) the data values between the groups (or within the sample, depending on the test). Calculate Statistic for Each Permutation: Compute the test statistic for each generated permutation. Construct Null Distribution: The distribution of these permuted test statistics forms the empirical null distribution. Calculate p-value: Determine the proportion of permuted statistics that are as extreme as, or more extreme than, the observed statistic. Practicality: Due to its computational intensity, this test is generally easier to perform using software (like R) rather than manually, even for moderate sample sizes [10.20, 491]. Keep these notes concise and focus on the key components. Practice applying these principles with exam-style questions to solidify your understanding. You’ve got this! R Practice "],["goodness-of-fit.html", "Chapter 12 Goodness of Fit Learning Objectives Theory R Practice", " Chapter 12 Goodness of Fit Learning Objectives Use a chi-square test to test the hypothesis that a random sample is from a particular distribution, including cases where parameters are unknown. Explain what is meant by a contingency (or two-way) table, and use a chi-square test to test the independence of two classification criteria. Theory R Practice "],["linear-regression.html", "Chapter 13 Linear Regression Learning Objectives Theory R Practice", " Chapter 13 Linear Regression Learning Objectives Explain what is meant by response and explanatory variables. State the simple regression model (with a single explanatory variable). Derive the least squares estimates of the slope and intercept parameters in a simple linear regression model. Use R to fit a simple linear regression model to a data set and interpret the output. Perform statistical inference on the slope parameter. Describe the use of measures of goodness of fit of a linear regression model. Use a fitted linear relationship to predict a mean response or an individual response with confidence limits. Use residuals to check the suitability and validity of a linear regression model. State the multiple linear regression model (with several explanatory variables). Use R to fit a multiple linear regression model to a data set and interpret the output. Use measures of model fit to select an appropriate set of explanatory variables. Theory Chapter: Linear Regression This chapter delves into quantifying and modelling relationships between variables, specifically focusing on linear associations. You’ll learn how to construct, fit, and assess linear regression models. 13.0.0.1 Learning Objective 1: Explain what is meant by response and explanatory variables. Response Variable (Y): This is the variable whose values are thought to depend on, or be explained by, another variable [CS1 Summary, Q3, 421]. It is the outcome or dependent variable in the model [CS1 CMP 2023.pdf, 139]. Explanatory Variable (X): This is the variable that is thought to influence or explain the values of the response variable [CS1 Summary, Q3, 421]. It is also known as the predictor or independent variable [CS1 CMP 2023.pdf, 139]. Ideally, its values are under the experimenter’s control [CS1 Summary, Q3, 421-422]. 13.0.0.2 Learning Objective 2: State the simple regression model (with a single explanatory variable). The simple linear regression model describes the relationship between a single response variable (Y) and a single explanatory variable (X) [CS1 Summary, Q1, 419]. Model Equation: \\(Y_i = \\alpha + \\beta X_i + \\epsilon_i\\) [CS1 Summary, Q1, 419]. \\(Y_i\\): The \\(i\\)-th observation of the response variable [CS1 Summary, Q1, 419]. \\(X_i\\): The \\(i\\)-th observation of the explanatory variable [CS1 Summary, Q1, 419]. \\(\\alpha\\): The intercept parameter (the expected value of Y when X is 0) [CS1 Summary, Q5, 424]. \\(\\beta\\): The slope parameter (the expected change in Y for a one-unit increase in X) [CS1 Summary, Q5, 424]. \\(\\epsilon_i\\): The error term for the \\(i\\)-th observation [CS1 Summary, Q1, 420]. Assumptions (Basic Model): Errors (\\(\\epsilon_i\\)) are uncorrelated [CS1 Summary, Q1, 420]. Expected value of errors: \\(E[\\epsilon_i] = 0\\) [CS1 Summary, Q1, 420]. Constant variance of errors: \\(Var(\\epsilon_i) = \\sigma^2\\) [CS1 Summary, Q1, 420]. Full Normal Model: In addition to the basic assumptions: Errors (\\(\\epsilon_i\\)) are independent and normally distributed: \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) [CS1 Summary, Q1, 420]. Consequently, \\(Y_i \\sim N(\\alpha + \\beta X_i, \\sigma^2)\\) [CS1 Summary, Q1, 420]. 13.0.0.3 Learning Objective 3: Derive the least squares estimates of the slope and intercept parameters in a simple linear regression model. The least squares method aims to find the \\(\\alpha\\) and \\(\\beta\\) that minimise the sum of squared residuals (errors) [CS1 Summary, Q4, 422]. * Objective: Minimise \\(\\sum \\epsilon_i^2 = \\sum (Y_i - \\alpha - \\beta X_i)^2\\) [CS1 Summary, Q4, 422]. * Derivation Steps: 1. Differentiate the sum of squared errors with respect to \\(\\alpha\\) and set the partial derivative to zero [CS1 Summary, Q4, 422]. 2. Differentiate the sum of squared errors with respect to \\(\\beta\\) and set the partial derivative to zero [CS1 Summary, Q4, 422]. 3. Solve the resulting two equations simultaneously to obtain the estimates [CS1 Summary, Q4, 423]. * Least Squares Estimates: * Slope (\\(\\hat{\\beta}\\)): \\(\\hat{\\beta} = s_{xy} / s_{xx}\\) [CS1 Summary, Q5, 424]. * Intercept (\\(\\hat{\\alpha}\\)): \\(\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}\\) [CS1 Summary, Q5, 424]. * Estimated Error Variance (\\(\\hat{\\sigma}^2\\)): \\(\\hat{\\sigma}^2 = \\frac{1}{n-2} \\left(s_{yy} - \\frac{s_{xy}^2}{s_{xx}}\\right)\\) [CS1 Summary, Q5, 424]. * Note: The fitted regression line \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) always passes through the point \\((\\bar{x}, \\bar{y})\\) [CS1 Summary, Q5b, 426]. * The terms \\(s_{xx}\\), \\(s_{yy}\\), \\(s_{xy}\\) are sum of squares and products, found on page 24 of the Tables and in CS1 Summary [CS1 Summary, Q2, 420-421]. 13.0.0.4 Learning Objective 4: Use R to fit a simple linear regression model to a data set and interpret the output. Fitting a Model in R: Use the lm() function: &lt;name_of_model&gt; &lt;- lm(&lt;response_variable&gt; ~ &lt;explanatory_variable&gt;) [Copy of CS1B Summary 4 - Regression 2020.pdf, 539]. To display coefficients: &lt;name_of_model&gt; [Copy of CS1B Summary 4 - Regression 2020.pdf, 539]. For comprehensive output (estimates, standard errors, t-values, p-values): summary(&lt;name_of_model&gt;) [Copy of CS1B Summary 4 - Regression 2020.pdf, 540]. Perform Statistical Inference on the Slope Parameter (\\(\\beta\\)): Test Statistic: Under the full normal model, \\(\\frac{\\hat{\\beta} - \\beta}{\\hat{\\sigma}/\\sqrt{s_{xx}}} \\sim t_{n-2}\\) [CS1 Summary, Q8, 431]. Hypotheses for testing \\(\\beta = 0\\) (no linear relationship): \\(H_0: \\beta = 0\\) \\(H_1: \\beta \\neq 0\\) (or one-sided alternatives) [CS1 Summary, Q12, 440]. Interpretation: The summary() output provides the t-value and p-value for testing if \\(\\beta\\) is significantly different from zero. A low p-value (e.g., &lt; 0.05) indicates sufficient evidence to reject \\(H_0\\), suggesting a significant linear relationship [CS1 Summary, Q8b, 434]. Confidence Intervals: Use confint(&lt;name_of_model&gt;, level=0.95) to get CIs for slope and intercept [Copy of CS1B Summary 4 - Regression 2020.pdf, 541]. Describe the Use of Measures of Goodness of Fit of a Linear Regression Model: Coefficient of Determination (\\(R^2\\)): Defined as \\(R^2 = \\frac{SS_{REG}}{SS_{TOT}}\\) [CS1 Summary, Q7, 429]. Measures the proportion of the total variation in the response variable (Y) that is explained by the regression model [CS1 Summary, Q7, 429]. Ranges from 0 to 1, often quoted as a percentage [CS1 Summary, Q7, 429]. Higher \\(R^2\\) indicates a better fit [CS1 Summary, Q7a, 430]. It is equal to the square of Pearson’s sample correlation coefficient, \\(R^2 = r^2\\) [CS1 Summary, Q7, 429]. ANOVA (Analysis of Variance) Table: Partitions the total variation (\\(SS_{TOT}\\)) into variation explained by the regression (\\(SS_{REG}\\)) and unexplained residual variation (\\(SS_{RES}\\)) [CS1 Summary, Q6, 426]. \\(SS_{TOT} = SS_{REG} + SS_{RES}\\) [CS1 Summary, Q6, 426]. Used to perform an F-test for the overall significance of the regression model (i.e., whether \\(\\beta=0\\)) [CS1 Summary, Q12, 440]. A large F-statistic and small p-value lead to rejection of \\(H_0\\) [CS1 Summary, Q12, 442]. anova(&lt;name_of_model&gt;) in R provides the ANOVA table [Copy of CS1B Summary 4 - Regression 2020.pdf, 540]. Use a Fitted Linear Relationship to Predict a Mean Response or an Individual Response with Confidence Limits: Mean Response (\\(\\hat{\\mu}_0\\)): An estimate of the average Y value for a given X value (\\(X_0\\)) [CS1 Summary, Q19, 451]. Formula: \\(\\hat{\\mu}_0 = \\hat{\\alpha} + \\hat{\\beta}X_0\\) [CS1 Summary, Q19, 451]. Confidence interval for the mean response: Accounts for uncertainty in estimating \\(\\alpha\\) and \\(\\beta\\) [CS1 Summary, Q10, 437]. Individual Response (\\(\\hat{y}_0\\)): A prediction for a single future Y observation at a given X value (\\(X_0\\)) [CS1 Summary, Q19, 452]. Formula: \\(\\hat{y}_0 = \\hat{\\alpha} + \\hat{\\beta}X_0\\) [CS1 Summary, Q19, 452]. Prediction interval for an individual response: Accounts for both estimation uncertainty AND the inherent variability of individual observations [CS1 Summary, Q10, 437]. Thus, prediction intervals are always wider than confidence intervals for the mean [CS1-AS, Q26 (iii)(b), 104]. In R: predict(&lt;name_of_model&gt;, &lt;new_data_frame&gt;, interval=\"confidence\", level=0.9) for mean response CI, and interval=\"predict\" for individual response PI [Copy of CS1B Summary 4 - Regression 2020.pdf, 541-542]. Use Residuals to Check the Suitability and Validity of a Linear Regression Model: Residuals (\\(\\hat{\\epsilon}_i\\) or \\(e_i\\)): The difference between the observed value and the fitted value: \\(\\hat{e}_i = Y_i - \\hat{Y}_i\\) [CS1 Summary, Q13, 442]. Checks: Residuals should be small, patternless, and normally distributed [CS1 Summary, Q13, 442]. Plot residuals vs. fitted values (or explanatory variable): plot(&lt;name_of_model&gt;, 1) in R [Copy of CS1B Summary 4 - Regression 2020.pdf, 542]. Look for: Patternlessness: Indicates independence of errors [CS1 Summary, Q13c, 445]. Constant Variance (Homoscedasticity): The spread of residuals should be consistent across the range of fitted values [CS1 Summary, Q13c, 445]. Normal Q-Q Plot of residuals: plot(&lt;name_of_model&gt;, 2) in R [Copy of CS1B Summary 4 - Regression 2020.pdf, 542]. Points should lie approximately along a straight diagonal line, indicating normality of errors [CS1 Summary, Q13b, 444]. Deviations suggest non-normality. Summary of residuals: summary(&lt;name_of_model&gt;) provides min, max, median, quartiles of residuals [Copy of CS1B Summary 4 - Regression 2020.pdf, 546]. 13.0.0.5 Learning Objective 5: State the multiple linear regression model (with several explanatory variables). The multiple linear regression model extends the simple linear model to include multiple explanatory variables [CS1 Summary, Q14, 445]. * Model Equation: \\(Y_i = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_k X_{ik} + \\epsilon_i\\) [CS1 Summary, Q14, 445]. * \\(Y_i\\): Response variable for the \\(i\\)-th observation [CS1 Summary, Q14, 445]. * \\(X_{i1}, \\dots, X_{ik}\\): \\(k\\) explanatory variables for the \\(i\\)-th observation [CS1 Summary, Q14, 445]. * \\(\\alpha\\): Intercept [CS1 Summary, Q14, 445]. * \\(\\beta_1, \\dots, \\beta_k\\): Slope parameters for each explanatory variable [CS1 Summary, Q14, 445]. * \\(\\epsilon_i\\): Error term [CS1 Summary, Q14, 446]. * Full Normal Model Assumptions: Similar to simple linear regression, the errors \\(\\epsilon_i\\) are independent and normally distributed: \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) [CS1 Summary, Q14, 446]. 13.0.0.6 Learning Objective 6: Use R to fit a multiple linear regression model to a data set and interpret the output. Fitting a Model in R: Use the lm() function, listing all explanatory variables separated by +: &lt;name_of_model&gt; &lt;- lm(&lt;response_variable&gt; ~ &lt;X1&gt; + &lt;X2&gt; + ...) [Copy of CS1B Summary 4 - Regression 2020.pdf, 543]. For interactions, use X1:X2 or X1*X2 (which includes main effects and interaction) [Copy of CS1B Summary 4 - Regression 2020.pdf, 544]. Output is accessed via summary(&lt;name_of_model&gt;) [Copy of CS1B Summary 4 - Regression 2020.pdf, 544]. Interpreting Output: Coefficients Table: Provides estimates, standard errors, t-values, and p-values for \\(\\alpha\\) and each \\(\\beta_j\\) [CS1 Summary, Q18a, 450; Copy of CS1B Summary 4 - Regression 2020.pdf, 544]. A low p-value for a \\(\\beta_j\\) indicates that \\(X_j\\) is a significant predictor after accounting for other variables in the model [CS1 Summary, Q18, 449]. R-squared (\\(R^2\\)) and Adjusted R-squared (Adjusted \\(R^2\\)): \\(R^2\\) is the proportion of variability explained by the model [Copy of CS1B Summary 4 - Regression 2020.pdf, 545]. Adjusted \\(R^2\\) adjusts \\(R^2\\) for the number of predictors, penalising models with more parameters [CS1 Summary, Q17, 448]. It is generally preferred for model comparison [CS1 Summary, Q17, 448]. F-statistic and overall p-value: Tests the global null hypothesis that all slope parameters are zero (\\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\)) [CS1 Summary, Q20, 453]. A low p-value indicates that at least one explanatory variable is a significant predictor [CS1 Summary, Q20, 455]. Confidence intervals for parameters: confint(&lt;name_of_model&gt;, level=0.95) [Copy of CS1B Summary 4 - Regression 2020.pdf, 545]. 13.0.0.7 Learning Objective 7: Use measures of model fit to select an appropriate set of explanatory variables. Selecting the optimal set of explanatory variables is crucial for building parsimonious yet accurate models. * Adjusted R-squared (\\(R^2_{adj}\\)): * As noted above, it accounts for the number of predictors [CS1 Summary, Q17, 448]. * A higher \\(R^2_{adj}\\) generally indicates a better model [CS1 Summary, Q17, 448]. * Akaike Information Criterion (AIC): A measure that balances model fit and complexity; lower AIC indicates a better model [CS1 Summary, Q22, 482]. (Note: AIC is not explicitly detailed in the provided sources for linear regression, but it is mentioned for GLMs and variable selection generally. It is a common measure in model selection.) * Statistical Significance of Parameters: Include only covariates where the hypothesis that \\(\\beta_j=0\\) can be rejected (i.e., they are statistically significant) [CS1 Summary, Q18, 449]. * Stepwise Selection Methods: * Forward Selection: Start with a null model (intercept only) and iteratively add the covariate that most significantly improves the model (e.g., reduces AIC the most or causes a significant decrease in deviance) until no further significant improvement is found [CS1 Summary, Q22, 482; Copy of CS1B Summary 4 - Regression 2020.pdf, 546]. * Backward Elimination: Start with a full model (all available covariates) and iteratively remove the least significant covariate (e.g., highest p-value for \\(\\beta_j\\) coefficient, or causes AIC to reach a minimum) until all remaining covariates are statistically significant [CS1 Summary, Q23, 483]. These tools and techniques are vital for developing robust predictive models in actuarial science. Keep practicing! R Practice "],["generalised-linear-models.html", "Chapter 14 Generalised Linear Models Learning Objectives Theory R Practice", " Chapter 14 Generalised Linear Models Learning Objectives Define an exponential family of distributions. Show that the following distributions may be written in this form: binomial, Poisson, exponential, gamma, normal. State the mean and variance for an exponential family, and define the variance function and the scale parameter. Derive these quantities for the distributions above. Explain what is meant by the link function and the canonical link function, referring to the distributions above. Explain what is meant by a variable, a factor taking categorical values and an interaction term. Define the linear predictor, illustrating its form for simple models, including polynomial models and models involving factors. Define the deviance and scaled deviance and state how the parameters of a generalised linear model may be estimated. Describe how a suitable model may be chosen by using an analysis of deviance and by examining the significance of the parameters. Define the Pearson and deviance residuals and describe how they may be used. Apply statistical tests to determine the acceptability of a fitted model: Pearson’s chi-square test and the likelihood ratio test Fit a generalised linear model to a data set and interpret the output. Theory Generalised Linear Models (GLMs): Core Concepts &amp; Applications Learning Objective 1: Define an exponential family of distributions. Show that the following distributions may be written in this form: binomial, Poisson, exponential, gamma, normal. At the heart of GLMs lies the exponential family of distributions. Definition: A probability function (PF) or probability density function (PDF), f(y), belongs to the exponential family if it can be written in the form: f(y; θ, φ) = exp{ (yθ - b(θ))/a(φ) + c(y, φ) }. Here, θ is the natural parameter (a function of the mean, μ) and φ is the scale (or dispersion) parameter. Distributions in Exponential Family Form: Binomial (Y ~ Bin(n, μ)): Natural Parameter (θ): log(μ / (1-μ)). Function b(θ): -nlog(1-μ) or nlog(1 + e^θ). Function a(φ): 1/φ = 1/n. Scale Parameter (φ): n. Poisson (Y ~ Poi(μ)): PF: P(Y=y) = (e(-μ)μy) / y!. Can be rewritten as: exp{ y ln(μ) - μ - ln(y!) }. Natural Parameter (θ): ln(μ). Function b(θ): μ = e^θ. Function a(φ): 1. Scale Parameter (φ): 1. Exponential (Y ~ Exp(λ) or Exp(1/μ)): PDF: f(x) = λe^(-λx). Can be rewritten as: exp{ -λx + ln(λ) }. Natural Parameter (θ): -λ or -1/μ. Function b(θ): -ln(-θ) or ln(μ). Function a(φ): 1. Scale Parameter (φ): 1. Gamma (Y ~ Gamma(α, λ) or Gamma(α, μ)): PDF: (λ^α / Γ(α)) x(α-1)e(-λx). Natural Parameter (θ): -λ or -1/μ. Function b(θ): -α ln(-θ) or α ln(μ). Function a(φ): 1/α. Scale Parameter (φ): α. Normal (Y ~ N(μ, σ²)): PDF: (1/(σ√(2π)))exp(-(x-μ)²/(2σ²)). Natural Parameter (θ): μ. Function b(θ): μ². Function a(φ): σ². Scale Parameter (φ): σ². Learning Objective 2: State the mean and variance for an exponential family, and define the variance function and the scale parameter. Derive these quantities for the distributions above. For a distribution in the exponential family form exp{ (yθ - b(θ))/a(φ) + c(y, φ) }: * Mean: E[Y] = b’(θ). * Variance: Var[Y] = a(φ) b’’(θ). Variance Function (V(μ)): This is b’’(θ) expressed in terms of the mean μ. It shows the relationship between the mean and variance of a distribution. * Scale Parameter (φ): This is typically the parameter in the distribution other than the mean, often represented by a(φ). Derivations/Values for Distributions: Poisson: E[Y] = μ. Var[Y] = μ. Variance Function: V(μ) = μ. Exponential: E[Y] = 1/λ. Var[Y] = 1/λ². Variance Function: V(μ) = μ². Gamma: E[Y] = α/λ. Var[Y] = α/λ². Variance Function: V(μ) = μ²/α. Normal: E[Y] = μ. Var[Y] = σ². Variance Function: V(μ) = 1 (since b’’(θ)=1 and a(φ)=σ², so Var(Y)=σ²). Binomial: E[Y] = np. Var[Y] = np(1-p). Variance Function: V(μ) = μ(1-μ/n). Learning Objective 3: Explain what is meant by the link function and the canonical link function, referring to the distributions above. Link Function (g(μ)): The link function g(μ) connects the mean of the response variable μ = E[Y] to the linear predictor η. It must be invertible and differentiable. Canonical Link Function: A specific link function for each exponential family distribution that simplifies the estimation process. If not specified, the canonical link function is usually used. It directly relates the linear predictor η to the natural parameter θ (η = θ). Examples of Canonical Link Functions: Poisson: log(μ). This ensures the mean μ is always positive (μ = e^η). Normal: identity (μ). Exponential: inverse (-1/μ). Binomial: logit (log(μ/(1-μ))). This ensures the probability μ is between 0 and 1 (μ = e^η / (1 + e^η)). Gamma: inverse (-1/μ). Learning Objective 4: Explain what is meant by a variable, a factor taking categorical values and an interaction term. Define the linear predictor, illustrating its form for simple models, including polynomial models and models involving factors. Linear Predictor (η): A function of the covariates that is linear in the parameters. It’s what we believe influences the response (e.g., premium, claim numbers). Types of Covariates: Variable: A covariate whose actual numerical value directly enters the linear predictor. Example: Age, duration. Linear predictor form: α + βx. Factor: A covariate that takes categorical values. Example: Sex (male/female), vehicle rating group. A parameter is assigned for each category. Form: If ‘Sex’ is a factor (Male, Female), the linear predictor might include α_i where i is Male or Female. Interaction Term: Occurs when one explanatory variable’s effect on the response depends on the value of another explanatory variable. Form: An interaction between X1 and X2 is denoted X1:X2 or included in X1X2, adding a term like γX1X2 to the linear predictor. Illustrative Linear Predictor Forms: Simple Linear Model: η = α + βX. Polynomial Model: η = α + βX + γX² (linear in parameters α, β, γ). Models with Factors: Age + Sex: η = α_i + βx (where i represents sex category). Age Sex:* η = α_i + β_i x (allows different slopes for each sex category). Learning Objective 5: Define the deviance and scaled deviance and state how the parameters of a generalised linear model may be estimated. Describe how a suitable model may be chosen by using an analysis of deviance and by examining the significance of the parameters. Parameter Estimation (Maximum Likelihood Estimates - MLEs): For GLMs, parameters in the linear predictor are typically estimated using the method of Maximum Likelihood Estimation (MLE). This involves: Obtaining the log-likelihood function of the observed data in terms of the means (μ). Rearranging the link function to express μ in terms of the linear predictor (η). Substituting this into the log-likelihood to express it in terms of the parameters of the linear predictor (α, β, etc.). Differentiating the log-likelihood with respect to each parameter and setting the derivatives to zero to find the MLEs. Deviance (D) and Scaled Deviance (D*): Measures the goodness-of-fit of a GLM. Scaled Deviance (D*): D* = 2 (lnL_SAT - lnL_M), where lnL_SAT* is the log-likelihood of the saturated model (perfect fit) and lnL_M is the log-likelihood of the current model. Deviance (D): D = D* φ, where φ* is the dispersion parameter. A smaller deviance indicates a better fit. The scaled deviance asymptotically follows a Chi-squared (χ²) distribution, particularly if the Y values are normally distributed. Model Selection: Analysis of Deviance: Increasing the number of parameters in a model always reduces the deviance. To formally compare nested models, calculate the difference in scaled deviances between the two models. This difference follows a χ² distribution with degrees of freedom equal to the difference in the number of parameters between the models. The anova() function in R can be used, often with test=\"Chisq\" for non-normal distributions, to perform these tests. Significance of Parameters: Examine the p-values for each parameter in the model output (e.g., from summary() in R). Parameters with small p-values (e.g., &lt; 0.05) are considered statistically significant and contribute meaningfully to the model. Akaike’s Information Criterion (AIC): AIC is a penalized log-likelihood that balances model fit and complexity. Lower AIC values indicate preferred models. Forward and Backward Selection: These are systematic approaches to add or remove variables from the model based on criteria like AIC or deviance tests. Learning Objective 6: Define the Pearson and deviance residuals and describe how they may be used. Residuals (General): The difference between the actual observed responses (y_i) and the fitted responses (*μ̂_i). e_i = y_i - μ̂_i*. Raw residuals are not directly used for checking due to unknown distributions. Pearson Residuals: Definition: *(y_i - μ̂_i) / √V(μ̂_i). Where V(μ̂_i)* is the estimated variance function. Primarily used for normally distributed data, as they are then approximately N(0,1). For non-normal data, they can be skewed and hard to interpret. Deviance Residuals: Definition: *sign(y_i - μ̂_i) * √d_i, where d_i* is the i-th component of the scaled deviance. Suitable for all distributions within the exponential family. They tend to be symmetrically distributed and approximately normal, even for non-normal data, making them preferred for actuarial applications. How Residuals are Used to Check Model Fit: Plots of residuals vs. fitted values/covariates: Used to check for randomness and constant variance. A patternless plot indicates a good fit. Q-Q plots (Quantile-Quantile plots) or Histograms of residuals: Used to check for normality of errors. Deviations from a straight line on a Q-Q plot suggest non-normality. Learning Objective 7: Apply statistical tests to determine the acceptability of a fitted model: Pearson’s chi-square test and the likelihood ratio test. Pearson’s Chi-Square (χ²) Test: Goodness-of-Fit: Used to assess if observed frequencies conform to a specified distribution (e.g., fair die, Poisson, binomial). Example: CS1-AS, Q45, 47, 48, 59, 62 illustrate checking if observed data fits a Poisson or binomial model. Independence (Contingency Tables): Used to test for association between two categorical variables. Example: CS1-AS, Q51, 60, 61 demonstrate testing for association between marital status and BMI, or characteristics of HIV positive men. The test statistic follows a χ² distribution. Likelihood Ratio Test (LRT): Often used for comparing nested GLMs. It is directly related to the difference in deviances. Test statistic: Difference in scaled deviances between two models (*D*_Model A - D*_Model B*). This statistic follows a χ² distribution with degrees of freedom equal to the difference in the number of parameters between the two models. Example: CS1-AS, Q47(v) compares Model A and Model B using the difference in their scaled deviances to determine if one is a significant improvement. Learning Objective 8: Fit a generalised linear model to a data set and interpret the output. Fitting a GLM (e.g., using R): The glm() function is commonly used. Specify the response variable, explanatory covariates, and the family (e.g., gaussian, binomial, Gamma, poisson) and link function (e.g., identity, log, inverse, logit). Example R code: &lt;name&gt; &lt;- glm(&lt;response&gt; ~ &lt;covariate1&gt; + &lt;covariate2&gt;, family=gaussian(link=\"identity\")). Interpreting Output (from summary(&lt;model_name&gt;)): Coefficients: Estimates for the parameters in the linear predictor (intercept and slopes for covariates). Standard Errors: Measure the precision of the coefficient estimates. Z or t Test Values: Used to assess the significance of individual parameters. Large absolute values suggest significance. P-values: Probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming the null hypothesis (parameter = 0) is true. Small p-values (e.g., &lt; 0.05) indicate that the parameter is statistically significant. Example: In CS1-AS, Q40(ii), p-values less than 0.05 indicate that ‘Intercept’, ‘drug_1’, and ‘drug_2’ are all significantly different from zero. Deviance/Scaled Deviance: Measures the goodness-of-fit of the overall model (as discussed in Objective 5). AIC: Aids in model selection (as discussed in Objective 5). Fitted Values: Estimates of the mean response for each data point. Obtained using fitted() or predict(..., type=\"response\"). Residuals: Can be extracted and plotted (e.g., using plot(&lt;model_name&gt;, 1) for residuals vs. fitted values, or plot(&lt;model_name&gt;, 2) for Q-Q plot) to check model assumptions. This comprehensive overview should give you a solid foundation for mastering GLMs in CS1. Keep practicing those derivations and interpretations – they’re key to exam success! R Practice "],["bayesian-statistics.html", "Chapter 15 Bayesian Statistics Learning Objectives Theory R Practice", " Chapter 15 Bayesian Statistics Learning Objectives Use Bayes’ theorem to calculate simple conditional probabilities. Explain what is meant by a prior distribution, a posterior distribution and a conjugate prior distribution. Derive the posterior distribution for a parameter in simple cases. Explain what is meant by a loss function. Use simple loss functions to derive Bayesian estimates of parameters. Explain what is meant by the credibility premium formula and describe the role played by the credibility factor. Explain the Bayesian approach to credibility theory and use it to derive credibility premiums in simple cases. Explain the empirical Bayes approach to credibility theory and use it to derive credibility premiums in simple cases. Explain the differences between the two approaches and state the assumptions underlying each of them. Theory Chapter: Bayesian Statistics 15.0.0.1 Learning Objectives Summary: Use Bayes’ theorem to calculate simple conditional probabilities. Definition: Bayes’ Theorem updates the probability of a hypothesis (H) given new evidence (E). For discrete events: P(H|E) = P(E|H)P(H) / P(E). For continuous distributions: posterior ∝ likelihood × prior. Application: It’s used to update beliefs about a parameter after observing data. Example (Discrete): To find the posterior distribution of a Poisson mean (λ) after observing claims, given a discrete prior distribution for λ. Explain what is meant by a prior distribution, a posterior distribution and a conjugate prior distribution. Prior Distribution: Represents initial beliefs about a parameter before observing any data. It can be discrete or continuous. Posterior Distribution: Represents the updated beliefs about a parameter after observing data. It is the conditional distribution of the parameter given the data. The posterior PDF is proportional to the likelihood function multiplied by the prior PDF. Conjugate Prior Distribution: A prior distribution is “conjugate” if, when combined with the likelihood function, it results in a posterior distribution that belongs to the same family as the prior distribution. Examples: Gamma prior for a Poisson mean (λ). Beta prior for a binomial probability (p). Normal prior for a Normal mean (with known variance). Derive the posterior distribution for a parameter in simple cases. General Steps (Continuous Prior): Write down the prior PDF. Write down the likelihood function (probability/PDF of observed data). Use the formula: Posterior PDF ∝ Likelihood × Prior PDF. Identify the posterior distribution by its form (e.g., Gamma for λ, Beta for p, Normal for μ). General Steps (Discrete Prior): The prior and posterior distributions have the same support. Calculate the conditional probabilities for each possible parameter value using Bayes’ Theorem. Simple Cases Covered: Poisson mean (λ) with Gamma prior. Binomial probability (p) with Beta prior. Normal mean (μ) with Normal prior. Exponential parameter (θ) with a specific prior. Explain what is meant by a loss function. Definition: A loss function is a measure of the penalty incurred when the chosen Bayesian estimate differs from the true value of the parameter. It quantifies the cost of making an estimation error. Use simple loss functions to derive Bayesian estimates of parameters. Squared Error Loss: Formula: Loss = (θ̂ - θ)^2. Bayesian Estimate: The mean of the posterior distribution. Absolute Error Loss: Formula: Loss = |θ̂ - θ|. Bayesian Estimate: The median of the posterior distribution. 0/1 (All-or-Nothing) Loss: Formula: Loss = 0 if θ̂ = θ, Loss = 1 if θ̂ ≠ θ. Bayesian Estimate: The mode of the posterior distribution (the value that maximises the posterior PDF). Explain what is meant by the credibility premium formula and describe the role played by the credibility factor. Credibility Premium: A premium that combines direct data from a specific risk (or group) with collateral data from similar risks (or a broader population). Formula: Credibility Premium = Z * x̄ + (1 - Z) * μ₀. x̄: Estimate based on the direct data (sample mean). μ₀: Prior or collective mean (based on collateral data/prior expectation). Z: The Credibility Factor. Role of Credibility Factor (Z): Determines the weight given to the individual risk’s experience (x̄) versus the broader population’s experience (μ₀). Z ranges from 0 to 1. A higher Z means more weight is given to the direct data, reflecting greater confidence in the individual experience. Z increases as the amount of direct data (n) increases, as more direct data provides a more reliable estimate from the individual experience. Z decreases as the reliability (or precision) of the prior/collective information increases (e.g., as the variance of the prior distribution decreases). Explain the Bayesian approach to credibility theory and use it to derive credibility premiums in simple cases. Bayesian Approach to Credibility: This approach formalises credibility by treating the underlying risk parameter as a random variable with a prior distribution. The “credibility premium” is derived as the mean of the posterior distribution (i.e., the Bayesian estimate under squared error loss). Derivation: The posterior mean is shown to be a weighted average of the sample mean (MLE) and the prior mean, which directly yields the credibility premium formula. Credibility Factor (Z) Examples: Poisson/Gamma Model: Z = n / (n + β). n: Number of observations (direct data). β: Parameter of the Gamma prior, related to the reliability of the prior information. Normal/Normal Model: Z = (n * σ₀²) / (n * σ₀² + σ²) or Z = n / (n + σ² / σ₀²). n: Number of observations. σ₀²: Variance of the prior distribution (reflects uncertainty in prior beliefs). σ²: Variance of the data within a risk group. Binomial/Beta Model: Z = n / (n + α + β). n: Sample size. α, β: Parameters of the Beta prior, related to prior reliability. Explain the empirical Bayes approach to credibility theory and use it to derive credibility premiums in simple cases. Empirical Bayes Credibility Theory (EBCT): This approach combines observed data with prior expectations to set premiums but, crucially, does not assume a specific parametric form for the prior distribution of the risk parameter (unlike full Bayesian credibility). Instead, the variance components needed for the credibility factor are estimated from the data itself. EBCT Model 1: Assumes observed claim amounts X_ij (for risk i in year j) are conditionally independent given a risk parameter θ_i. Estimates variance within risk groups E[s²(θ)] and variance between risk groups var[m(θ)] from the data. Credibility Factor (Z): Z = n / (n + E[s²(θ)] / var[m(θ)]). Credibility Premium: Z * x̄_i + (1 - Z) * x̄. x̄_i: Average claims for risk i. x̄: Overall average claims across all risks and years. EBCT Model 2: Extends Model 1 by incorporating a “risk volume” parameter (P_j) to account for different sizes of policies or businesses. This acknowledges that the number of observations can vary per risk. Credibility Factor (Z_i): Specific to each risk i and influenced by its risk volume. Credibility Premium: Z_i * x̄_i + (1 - Z_i) * x̄ (where x̄_i and x̄ are weighted averages by risk volume). Explain the differences between the two approaches and state the assumptions underlying each of them. Key Differences: Prior Distribution: Bayesian Credibility: Assumes a specific, often parametric, prior distribution for the risk parameter (e.g., Gamma, Normal, Beta). Empirical Bayes Credibility: Does not assume a specific prior distribution for the risk parameter. Instead, it estimates the necessary parameters for the credibility factor directly from the observed data. Parameter Estimation: Bayesian Credibility: Integrates the likelihood with the specified prior to derive the posterior distribution, from which the premium (posterior mean) is calculated. Empirical Bayes Credibility: Uses the observed data to estimate the moments (means and variances) of the underlying distributions of the claims, and then uses these empirical estimates in the credibility formula. Information Usage: Bayesian Credibility: Uses a pre-specified prior and the current sample data. Empirical Bayes Credibility: Uses the collective experience from all observed risks/years to estimate variance components, even if a specific prior is unknown, and then combines this with individual risk experience. Underlying Assumptions: Bayesian Credibility: The risk parameter (e.g., λ, p, μ) is a random variable. A specific parametric form for the prior distribution of the risk parameter is assumed. The observations within each risk group are conditionally independent and identically distributed given the risk parameter. Empirical Bayes Credibility: The risk parameter is a random variable, but its specific prior distribution is unknown or not explicitly modelled. Observations are conditionally independent given the risk parameter (X_ij | θ_i are iid). The variance of claims within a risk group may depend on the risk parameter (var(X_j|θ) = s²(θ)), for Model 1. For Model 2, different risk volumes (P_j) are incorporated, allowing for varying exposure. No assumption of normality for any random variables or parameters in the model (e.g., EBCT Model 1/2). Keep reviewing these core concepts, and remember, practice makes perfect for these types of derivations and explanations! R Practice "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
